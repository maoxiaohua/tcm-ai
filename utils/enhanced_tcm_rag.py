#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„ä¸­åŒ»RAGç³»ç»Ÿ
é›†æˆæ”¹è¿›çš„æ–‡æ¡£å¤„ç†å’Œæ£€ç´¢åŠŸèƒ½
"""

import os
import sys
import json
import pickle
import numpy as np
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/opt/tcm-ai')

from utils.tcm_document_processor import TCMDocumentProcessor
from core.knowledge_retrieval.enhanced_retrieval import EnhancedKnowledgeRetrieval

logger = logging.getLogger(__name__)

class EnhancedTCMRAG:
    """å¢å¼ºçš„ä¸­åŒ»RAGç³»ç»Ÿ"""
    
    def __init__(self, knowledge_db_path: str = "/opt/tcm-ai/knowledge_db"):
        self.knowledge_db_path = knowledge_db_path
        self.documents_file = os.path.join(knowledge_db_path, "tcm_documents.pkl")
        self.metadata_file = os.path.join(knowledge_db_path, "tcm_metadata.pkl")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.doc_processor = TCMDocumentProcessor()
        self.enhanced_retrieval = EnhancedKnowledgeRetrieval(knowledge_db_path)
        
        # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
        os.makedirs(knowledge_db_path, exist_ok=True)
        
        # åŠ è½½å·²å¤„ç†çš„æ–‡æ¡£
        self.documents = []
        self.metadata = []
        self.load_processed_documents()
        
    def load_processed_documents(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡æ¡£"""
        try:
            if os.path.exists(self.documents_file):
                with open(self.documents_file, 'rb') as f:
                    self.documents = pickle.load(f)
                    
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'rb') as f:
                    self.metadata = pickle.load(f)
                    
            logger.info(f"åŠ è½½å·²å¤„ç†æ–‡æ¡£: {len(self.documents)}ä¸ªåˆ†å—")
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            self.documents = []
            self.metadata = []
    
    def save_processed_documents(self):
        """ä¿å­˜å¤„ç†åçš„æ–‡æ¡£"""
        try:
            with open(self.documents_file, 'wb') as f:
                pickle.dump(self.documents, f)
                
            with open(self.metadata_file, 'wb') as f:
                pickle.dump(self.metadata, f)
                
            logger.info(f"ä¿å­˜æ–‡æ¡£æˆåŠŸ: {len(self.documents)}ä¸ªåˆ†å—")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æ¡£å¤±è´¥: {e}")
    
    def process_docx_files(self, docx_files: List[str]) -> Dict[str, Any]:
        """å¤„ç†docxæ–‡ä»¶å¹¶æ›´æ–°çŸ¥è¯†åº“"""
        processing_result = {
            'success': False,
            'processed_files': [],
            'total_chunks': 0,
            'failed_files': [],
            'stats': {}
        }
        
        try:
            new_documents = []
            new_metadata = []
            
            for docx_file in docx_files:
                logger.info(f"å¤„ç†æ–‡ä»¶: {docx_file}")
                
                # å¤„ç†å•ä¸ªæ–‡æ¡£
                result = self.doc_processor.process_tcm_document(docx_file)
                
                if result['success']:
                    filename = result['filename']
                    chunks = result['chunks']
                    
                    # æ·»åŠ åˆ†å—åˆ°çŸ¥è¯†åº“
                    for i, chunk in enumerate(chunks):
                        # æ–‡æ¡£å†…å®¹
                        document_text = chunk['text']
                        new_documents.append(document_text)
                        
                        # å…ƒæ•°æ®
                        metadata = {
                            'source': filename,
                            'chunk_index': i,
                            'section_type': chunk['section_type'],
                            'char_count': chunk['char_count'],
                            'disease_category': self._extract_disease_category(filename),
                            'processing_timestamp': str(result.get('timestamp', ''))
                        }
                        new_metadata.append(metadata)
                    
                    processing_result['processed_files'].append({
                        'filename': filename,
                        'chunks_count': len(chunks),
                        'stats': result['stats']
                    })
                    
                    print(f"âœ… {filename}: {len(chunks)}ä¸ªåˆ†å—")
                    
                else:
                    processing_result['failed_files'].append({
                        'filename': os.path.basename(docx_file),
                        'error': result['error_message']
                    })
                    print(f"âŒ {os.path.basename(docx_file)}: {result['error_message']}")
            
            # æ›´æ–°æ–‡æ¡£åˆ—è¡¨
            self.documents.extend(new_documents)
            self.metadata.extend(new_metadata)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            self.save_processed_documents()
            
            processing_result.update({
                'success': True,
                'total_chunks': len(new_documents),
                'stats': {
                    'total_documents': len(self.documents),
                    'new_chunks': len(new_documents),
                    'avg_chunk_size': np.mean([len(doc) for doc in new_documents]) if new_documents else 0
                }
            })
            
        except Exception as e:
            processing_result['error'] = str(e)
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            
        return processing_result
    
    def _extract_disease_category(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–ç–¾ç—…ç±»åˆ«"""
        category_mapping = {
            'æ„Ÿå†’': 'å¤–æ„Ÿç–¾ç—…',
            'å’³å—½': 'å‘¼å¸ç³»ç»Ÿ',
            'é«˜è¡€å‹': 'å¿ƒè¡€ç®¡ç³»ç»Ÿ', 
            'ç³–å°¿ç—…': 'å†…åˆ†æ³Œç³»ç»Ÿ',
            'æœˆç»å¤±è°ƒ': 'å¦‡ç§‘ç–¾ç—…',
            'å°å„¿': 'å„¿ç§‘ç–¾ç—…',
            'è…°ç—›': 'éª¨ç§‘ç–¾ç—…',
            'å¤±çœ ': 'ç¥ç»ç²¾ç¥ç§‘',
            'æ¹¿ç–¹': 'çš®è‚¤ç§‘',
            'æ¶ˆåŒ–': 'æ¶ˆåŒ–ç³»ç»Ÿ'
        }
        
        for keyword, category in category_mapping.items():
            if keyword in filename:
                return category
        return 'å…¶ä»–'
    
    def search_knowledge(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """æœç´¢çŸ¥è¯†åº“"""
        search_result = {
            'success': False,
            'query': query,
            'results': [],
            'search_method': 'enhanced_hybrid',
            'total_candidates': len(self.documents)
        }
        
        try:
            if not self.documents:
                search_result['error'] = "çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
                return search_result
            
            # ä½¿ç”¨å¢å¼ºæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                # æ¨¡æ‹Ÿå‘é‡åµŒå…¥ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹ï¼‰
                query_embedding = self._generate_mock_embedding(query)
                
                # æ··åˆæ£€ç´¢
                enhanced_results = self.enhanced_retrieval.hybrid_search(
                    query=query,
                    query_embedding=query_embedding,
                    total_results=top_k
                )
                
                search_result['results'] = enhanced_results
                search_result['search_method'] = 'enhanced_hybrid'
                
            except Exception as e:
                logger.warning(f"å¢å¼ºæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢: {e}")
                # å›é€€åˆ°åŸºç¡€æ£€ç´¢
                basic_results = self._basic_search(query, top_k)
                search_result['results'] = basic_results
                search_result['search_method'] = 'basic_fallback'
            
            search_result['success'] = True
            
        except Exception as e:
            search_result['error'] = str(e)
            logger.error(f"çŸ¥è¯†æœç´¢å¤±è´¥: {e}")
            
        return search_result
    
    def _generate_mock_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ¨¡æ‹ŸåµŒå…¥å‘é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        # åŸºäºæ–‡æœ¬hashç”Ÿæˆå›ºå®šç»´åº¦çš„å‘é‡
        import hashlib
        text_hash = hashlib.md5(text.encode()).hexdigest()
        # è½¬æ¢ä¸º384ç»´å‘é‡ï¼ˆå¸¸è§çš„åµŒå…¥ç»´åº¦ï¼‰
        vector = []
        for i in range(0, len(text_hash), 2):
            hex_val = text_hash[i:i+2]
            vector.append(int(hex_val, 16) / 255.0)
        
        # å¡«å……åˆ°384ç»´
        while len(vector) < 384:
            vector.extend(vector[:min(len(vector), 384-len(vector))])
        
        return vector[:384]
    
    def _basic_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """åŸºç¡€å…³é”®è¯æœç´¢"""
        results = []
        query_lower = query.lower()
        
        for i, (doc, meta) in enumerate(zip(self.documents, self.metadata)):
            doc_lower = doc.lower()
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
            score = 0.0
            for word in query_lower.split():
                if word in doc_lower:
                    score += doc_lower.count(word) / len(doc_lower.split())
            
            if score > 0:
                results.append({
                    'document': doc,
                    'metadata': meta,
                    'score': score,
                    'index': i
                })
        
        # æŒ‰è¯„åˆ†æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def test_rag_system(self, test_queries: List[str]) -> Dict[str, Any]:
        """æµ‹è¯•RAGç³»ç»Ÿæ•ˆæœ"""
        test_result = {
            'success': False,
            'test_queries': test_queries,
            'results': [],
            'summary': {}
        }
        
        try:
            query_results = []
            
            for query in test_queries:
                print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
                
                search_result = self.search_knowledge(query, top_k=3)
                
                if search_result['success']:
                    print(f"âœ… æ‰¾åˆ° {len(search_result['results'])} ä¸ªç›¸å…³ç»“æœ")
                    
                    # æ˜¾ç¤ºå‰2ä¸ªç»“æœ
                    for i, result in enumerate(search_result['results'][:2]):
                        score = result.get('hybrid_score', result.get('score', 0))
                        source = result['metadata'].get('source', 'unknown')
                        section = result['metadata'].get('section_type', 'other')
                        content = result['document'][:100]
                        
                        print(f"  ç»“æœ{i+1} [è¯„åˆ†:{score:.3f}] æ¥æº:{source} éƒ¨åˆ†:{section}")
                        print(f"    å†…å®¹: {content}...")
                else:
                    print(f"âŒ æœç´¢å¤±è´¥: {search_result.get('error', 'unknown')}")
                
                query_results.append({
                    'query': query,
                    'search_result': search_result
                })
            
            test_result.update({
                'success': True,
                'results': query_results,
                'summary': {
                    'total_queries': len(test_queries),
                    'successful_queries': len([r for r in query_results if r['search_result']['success']]),
                    'avg_results_per_query': np.mean([len(r['search_result']['results']) for r in query_results])
                }
            })
            
        except Exception as e:
            test_result['error'] = str(e)
            logger.error(f"RAGæµ‹è¯•å¤±è´¥: {e}")
            
        return test_result

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºä¸­åŒ»RAGç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag_system = EnhancedTCMRAG()
    
    # æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    test_files = [
        '/opt/tcm-ai/all_tcm_docs/æ„Ÿå†’.docx',
        '/opt/tcm-ai/all_tcm_docs/é«˜è¡€å‹.docx',
        '/opt/tcm-ai/all_tcm_docs/å’³å—½.docx',
        '/opt/tcm-ai/all_tcm_docs/ç³–å°¿ç—….docx',
        '/opt/tcm-ai/all_tcm_docs/æœˆç»å¤±è°ƒ.docx'
    ]
    
    # è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
    existing_files = [f for f in test_files if os.path.exists(f)]
    print(f"ğŸ“ æ‰¾åˆ° {len(existing_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    
    # å¤„ç†æ–‡æ¡£
    print(f"\nğŸ“ å¼€å§‹å¤„ç†æ–‡æ¡£...")
    processing_result = rag_system.process_docx_files(existing_files)
    
    if processing_result['success']:
        stats = processing_result['stats']
        print(f"\nâœ… æ–‡æ¡£å¤„ç†å®Œæˆ!")
        print(f"   æ€»åˆ†å—æ•°: {stats['total_documents']}")
        print(f"   æ–°å¢åˆ†å—: {stats['new_chunks']}")
        print(f"   å¹³å‡åˆ†å—å¤§å°: {stats['avg_chunk_size']:.0f}å­—ç¬¦")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "æ„Ÿå†’çš„ç—‡çŠ¶æœ‰å“ªäº›",
            "é«˜è¡€å‹å¦‚ä½•æ²»ç–—", 
            "å’³å—½åƒä»€ä¹ˆè¯",
            "ç³–å°¿ç—…çš„ç—…å› ",
            "æœˆç»å¤±è°ƒçš„ä¸­åŒ»è°ƒç†"
        ]
        
        print(f"\nğŸ§ª å¼€å§‹RAGæ•ˆæœæµ‹è¯•...")
        test_result = rag_system.test_rag_system(test_queries)
        
        if test_result['success']:
            summary = test_result['summary']
            print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
            print(f"   æµ‹è¯•æŸ¥è¯¢æ•°: {summary['total_queries']}")
            print(f"   æˆåŠŸæŸ¥è¯¢æ•°: {summary['successful_queries']}")
            print(f"   å¹³å‡ç»“æœæ•°: {summary['avg_results_per_query']:.1f}")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {test_result.get('error', 'unknown')}")
            
    else:
        print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {processing_result.get('error', 'unknown')}")

if __name__ == "__main__":
    main()