# enhanced_retrieval.py - å¢å¼ºæ£€ç´¢ç²¾åº¦ç³»ç»Ÿ

import os
import re
import jieba
import numpy as np
import faiss
import pickle
from collections import defaultdict, Counter
from typing import List, Dict, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class EnhancedKnowledgeRetrieval:
    def __init__(self, knowledge_db_path: str):
        self.knowledge_db_path = knowledge_db_path
        self.faiss_index_file = os.path.join(knowledge_db_path, "knowledge.index")
        self.documents_file = os.path.join(knowledge_db_path, "documents.pkl")
        self.metadata_file = os.path.join(knowledge_db_path, "metadata.pkl")
        
        # æ··åˆæ£€ç´¢ç»„ä»¶
        self.faiss_index = None
        self.documents = []
        self.metadata = []
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
        # ç–¾ç—…ç²¾ç¡®åŒ¹é…æƒé‡å­—å…¸ - ç”¨äºæå‡ç–¾ç—…åç§°æŸ¥è¯¢ç²¾åº¦
        self.disease_exact_weights = {
            # ä¸»è¦ç–¾ç—… - é«˜æƒé‡
            "é«˜è¡€å‹": 10.0, "ç³–å°¿ç—…": 10.0, "å† å¿ƒç—…": 10.0, "è‚ºç»“æ ¸ç—…": 10.0,
            "æ¶ˆåŒ–æ€§æºƒç–¡": 9.0, "è‚ç¡¬åŒ–": 9.0, "æ…¢æ€§æ°”ç®¡ç‚": 9.0, "ç¥ç»è¡°å¼±": 9.0,
            "æœˆç»å¤±è°ƒ": 8.0, "è„‘è¡€ç®¡ç—…": 8.0, "è‚¾ç‚": 8.0, "å…³èŠ‚ç‚": 8.0,
            "è‚¿ç˜¤ç—…": 8.0, "è™šæŸ": 7.0, "å“®å–˜": 7.0, "æ¹¿ç–¹": 7.0,
            
            # å„¿ç§‘ç–¾ç—…
            "å°å„¿æ”¯æ°”ç®¡ç‚": 9.0, "å°å„¿æ¶ˆåŒ–ä¸è‰¯": 9.0, "å°å„¿é—å°¿ç—‡": 8.0,
            "å°å„¿ç—…æ¯’æ€§è‚ºç‚": 8.0, "å°å„¿ç—¿è¯": 7.0, "ç™¾æ—¥å’³": 7.0,
            "æµè¡Œæ€§è…®è…ºç‚": 7.0, "éº»ç–¹": 7.0, "é£ç–¹": 6.0, "æ°´ç—˜": 6.0,
            
            # å¦‡ç§‘ç–¾ç—…
            "åŠŸèƒ½æ€§å­å®«å‡ºè¡€": 8.0, "ç—›ç»": 7.0, "æµäº§": 7.0, 
            "å­å®«è„±å‚": 7.0, "ç¼ºä¹³": 6.0, "å¦Šå¨ å‘•å": 6.0,
            
            # å¤–ç§‘ç–¾ç—…
            "æ³Œå°¿ç³»ç»“çŸ³": 8.0, "èƒ†çŸ³ç—…": 8.0, "è…°ç—›": 7.0, "å¤´æ™•": 7.0,
            "è‚›é—¨ç›´è‚ ç—…": 7.0, "ä¹³æˆ¿ç–¾æ‚£": 6.0, "é¢ˆæ·‹å·´ç»“æ ¸": 6.0,
            
            # äº”å®˜ç§‘ç–¾ç—…
            "å’½ç‚": 7.0, "æ‰æ¡ƒä½“ç‚": 7.0, "åŒ–è„“æ€§ä¸­è€³ç‚": 7.0,
            "é’å…‰çœ¼": 6.0, "å¤å­£å¸¸è§çš®è‚¤ç—…": 6.0,
            
            # å†…ç§‘æ‚ç—…
            "æ„Ÿå†’": 8.0, "å’³å—½": 7.0, "ä¾¿ç§˜": 7.0, "æ³„æ³»": 7.0,
            "ç–Ÿç–¾": 6.0, "ç™”ç—…": 6.0, "ç™«ç—«": 6.0, "é£Ÿæ¬²ä¸æŒ¯": 6.0,
            "å‡ºæ±—å¼‚å¸¸": 6.0, "è¡€æ “é—­å¡æ€§è„‰ç®¡ç‚": 6.0, "è´«è¡€": 6.0,
            
            # è€å¹´ç—…å’Œå¸¸è§ç—…
            "è€å¹´ç—…": 7.0, "ä¸­æš‘": 6.0, "å¤§å¶æ€§è‚ºç‚": 7.0, "è‚ºè„“è‚¿": 7.0,
            "èƒ†é“è›”è™«ç—…": 6.0, "è„‘ç‚åé—ç—‡": 6.0, "å¤§è„‘å‘è‚²ä¸å…¨": 6.0,
            "å¸¸è§è‚ å¯„ç”Ÿè™«ç—…": 6.0, "å°å„¿è¥å…»ä¸è‰¯": 6.0, "ç”·å­ä¸è‚²": 6.0
        }
        
        # å¤§å¹…æ‰©å±•çš„ä¸­åŒ»æœ¯è¯­åŒä¹‰è¯å­—å…¸
        self.tcm_synonyms = {
            # åŸºç¡€ç—‡çŠ¶
            "å¤´ç—›": ["å¤´ç–¼", "è„‘è¢‹ç–¼", "åå¤´ç—›", "å¤´æ™•ç—›", "å¤´èƒ€ç—›", "å·…é¡¶ç—›"],
            "å‘çƒ­": ["å‘çƒ§", "é«˜çƒ­", "ä½çƒ­", "èº«çƒ­", "æ½®çƒ­", "å¯’çƒ­å¾€æ¥", "å¾®çƒ­"],
            "å’³å—½": ["å’³", "å¹²å’³", "æ¹¿å’³", "ä¹…å’³", "ç—°å’³", "å’³ç—°", "å’³é€†"],
            "å¤±çœ ": ["ä¸å¯", "ç¡ä¸ç€", "å¤šæ¢¦", "æ—©é†’", "å…¥ç¡å›°éš¾", "ç¥ç»è¡°å¼±å¤±çœ "],
            "ä¾¿ç§˜": ["å¤§ä¾¿å¹²", "æ’ä¾¿éš¾", "ä¾¿ç»“", "å¤§ä¾¿ä¸é€š", "ä¾¿éš¾"],
            "è…¹æ³»": ["æ³„æ³»", "æ‹‰è‚šå­", "å¤§ä¾¿æº", "æ°´æ ·ä¾¿", "æºæ³„", "æ»‘æ³„"],
            
            # ä¸»è¦ç–¾ç—…
            "é«˜è¡€å‹": ["è¡€å‹é«˜", "é«˜è¡€å‹ç—…", "çœ©æ™•", "å¤´æ™•ç›®çœ©"],
            "ç³–å°¿ç—…": ["æ¶ˆæ¸´", "æ¶ˆæ¸´ç—‡", "ä¸‰å¤šä¸€å°‘", "å¤šå°¿å¤šé¥®å¤šé£Ÿ"],
            "æœˆç»å¤±è°ƒ": ["æœˆç»ä¸è°ƒ", "ç»æœŸç´Šä¹±", "æœˆäº‹ä¸è°ƒ", "ç»æ°´ä¸è°ƒ", "ç»è¡€å¼‚å¸¸"],
            "è„‘è¡€ç®¡ç—…": ["ä¸­é£", "å’ä¸­", "åç˜«", "åŠèº«ä¸é‚", "è„‘æ¢—", "è„‘å‡ºè¡€"],
            "è‚ç¡¬åŒ–": ["è‚ç¡¬å˜", "è‚çº¤ç»´åŒ–", "ç§¯èš", "é¼“èƒ€"],
            "æ¶ˆåŒ–æ€§æºƒç–¡": ["èƒƒæºƒç–¡", "åäºŒæŒ‡è‚ æºƒç–¡", "èƒƒè„˜ç—›", "å¿ƒä¸‹ç—›"],
            "è‚ºç»“æ ¸ç—…": ["è‚ºç—¨", "ç—¨ç—…", "è‚ºç—…", "å’¯è¡€"],
            "æ…¢æ€§æ°”ç®¡ç‚": ["æ°”ç®¡ç‚", "æ”¯æ°”ç®¡ç‚", "æ…¢æ”¯", "å’³å–˜"],
            "ç¥ç»è¡°å¼±": ["ç¥ç»è¡°å¼±å¤±çœ ", "å¿ƒç¥ä¸å®", "å¿ƒæ‚¸å¤±çœ ", "å¥å¿˜"],
            
            # å¦‡ç§‘ç–¾ç—…
            "æµäº§": ["å°äº§", "èƒå •", "èƒåŠ¨ä¸å®‰", "èƒæ¼"],
            "å­å®«è„±å‚": ["é˜´æŒº", "é˜´è„±", "å­å®«ä¸‹å‚"],
            "ç™½å¸¦å¼‚å¸¸": ["å¸¦ä¸‹", "èµ¤ç™½å¸¦", "å¸¦ä¸‹è¿‡å¤š", "é˜´é“åˆ†æ³Œç‰©"],
            
            # è‚¿ç˜¤ç›¸å…³
            "è‚¿ç˜¤ç—…": ["è‚¿ç˜¤", "ç™Œç—‡", "æ¶æ€§è‚¿ç˜¤", "ç§¯èš", "ç—‡ç˜•"],
            "ä¹³è…ºè‚¿å—": ["ä¹³ç™–", "ä¹³ä¸­ç»“æ ¸", "ä¹³æˆ¿è‚¿å—"],
            
            # å†…ç§‘ç–¾ç—…  
            "è™šæŸ": ["è™šåŠ³", "ä½“è™š", "æ°”è¡€è™š", "ç²¾ç¥ä¸æŒ¯", "ä¹åŠ›"],
            "å‡ºæ±—å¼‚å¸¸": ["è‡ªæ±—", "ç›—æ±—", "æ±—å‡º", "å¤šæ±—", "æ— æ±—"],
            "æ°´è‚¿": ["è‚¿èƒ€", "æµ®è‚¿", "èº«è‚¿", "é¢è‚¿", "è¶³è‚¿"],
            "é»„ç–¸": ["èº«é»„", "ç›®é»„", "å°ä¾¿é»„", "é˜³é»„", "é˜´é»„"],
            
            # å‘¼å¸ç³»ç»Ÿ
            "å’½ç‚": ["å’½ç—›", "å–‰ç—›", "å’½å¹²", "å’½éƒ¨ä¸é€‚"],
            "æ‰æ¡ƒä½“ç‚": ["ä¹³è›¾", "å–‰è›¾", "æ‰æ¡ƒä½“è‚¿å¤§"],
            "å“®å–˜": ["å–˜è¯", "æ°”å–˜", "å“®å¼", "å–˜æ¯"],
            
            # æ¶ˆåŒ–ç³»ç»Ÿ
            "èƒƒç‚": ["èƒƒè„˜ç—›", "èƒƒç—›", "å¿ƒä¸‹ç—›", "èƒƒèƒ€"],
            "è‚ç‚": ["èƒç—›", "è‚åŒºç—›", "é»„ç–¸", "è‚ç—…"],
            "èƒ†å›Šç‚": ["èƒ†èƒ€", "å³èƒç—›", "èƒ†çŸ³ç—‡"],
            "è‚ ç‚": ["è…¹ç—›", "è‚ é¸£", "æ³„æ³»", "ç—¢ç–¾"],
            
            # æ³Œå°¿ç³»ç»Ÿ
            "è‚¾ç‚": ["æ°´è‚¿", "è…°ç—›", "å°ä¾¿ä¸åˆ©", "è‚¾ç—…"],
            "å°¿è·¯æ„ŸæŸ“": ["æ·‹è¯", "å°ä¾¿æ¶©ç—›", "å°¿é¢‘å°¿æ€¥"],
            "å‰åˆ—è…ºç‚": ["ç²¾æµŠ", "ç™½æµŠ", "å‰é˜´ä¸é€‚"],
            
            # éª¨ç§‘ç–¾ç—…
            "é¢ˆæ¤ç—…": ["é¡¹å¼º", "é¢ˆé¡¹å¼ºç—›", "è½æ•"],
            "è…°æ¤ç—…": ["è…°ç—›", "è…°è„Šç—›", "è…°è†é…¸è½¯"],
            "å…³èŠ‚ç‚": ["ç—¹è¯", "å…³èŠ‚ç—›", "éª¨ç—¹", "è¡Œç—¹", "ç—›ç—¹", "ç€ç—¹"],
            
            # çš®è‚¤ç–¾ç—…
            "æ¹¿ç–¹": ["æ¹¿ç–®", "æµ¸æ·«ç–®", "å››å¼¯é£"],
            "è¨éº»ç–¹": ["ç˜¾ç–¹", "é£ç–¹å—", "é£å›¢"],
            "é“¶å±‘ç—…": ["ç™½ç–•", "æ¾çš®ç™£", "ç‰›çš®ç™£"],
            
            # äº”å®˜ç§‘
            "ç»“è†œç‚": ["ç›®èµ¤", "çº¢çœ¼ç—…", "é£çƒ­çœ¼", "å¤©è¡Œèµ¤çœ¼"],
            "ä¸­è€³ç‚": ["è€³ç—›", "è„“è€³", "è¤è€³"],
            "é¼»ç‚": ["é¼»å¡", "é¼»æ¸Š", "é¼»çª’"],
            
            # è€å¹´ç–¾ç—…
            "è€å¹´ç—…": ["è€å¹´ç»¼åˆå¾", "è¡°è€", "ç²¾ç¥èé¡", "ä½“å¼±"],
            "åŠ¨è„‰ç¡¬åŒ–": ["è¡€è„‰ç˜€é˜»", "è¡€ç®¡ç¡¬åŒ–", "è„‰é“ä¸åˆ©"],
            "å† å¿ƒç—…": ["èƒ¸ç—¹", "å¿ƒç—›", "çœŸå¿ƒç—›", "å¥å¿ƒç—›"],
            
            # å„¿ç§‘ç–¾ç—…
            "å°å„¿ç—…æ¯’æ€§è‚ºç‚": ["å°å„¿è‚ºç‚", "å„¿ç«¥è‚ºç‚", "å°å„¿å’³å–˜", "å°å„¿å‘çƒ­å’³å—½"],
            "å°å„¿è…¹æ³»": ["å°å„¿æ³„æ³»", "å©´å¹¼å„¿è…¹æ³»", "å°å„¿æ¶ˆåŒ–ä¸è‰¯"],
            "å°å„¿æ„Ÿå†’": ["å°å„¿å¤–æ„Ÿ", "å„¿ç«¥æ„Ÿå†’", "å°å„¿å‘çƒ­"],
            
            # ç²¾ç¥ç¥ç»
            "æŠ‘éƒç—‡": ["éƒè¯", "æƒ…å¿—ä¸ç•…", "å¿ƒæƒ…æŠ‘éƒ", "æ‚²ä¼¤"],
            "ç„¦è™‘ç—‡": ["æƒŠæ‚¸", "å¿ƒç¥ä¸å®", "çƒ¦èºä¸å®‰"],
            "å¥å¿˜": ["å–„å¿˜", "è®°å¿†åŠ›å‡é€€", "ç¥å¿—ä¸æ¸…"],
            
            # ä»£è°¢ç–¾ç—…
            "ç—›é£": ["ç—¹è¯", "ç™½è™å†èŠ‚é£", "å†èŠ‚ç—…"],
            "ç”²äº¢": ["ç˜¿ç—…", "å¿ƒæ‚¸å¤šæ±—", "ç”²çŠ¶è…ºè‚¿å¤§"],
            "ç”²å‡": ["è™šåŠ³", "ç•å¯’ä¹åŠ›", "ç”²çŠ¶è…ºåŠŸèƒ½å‡é€€"]
        }
        
        # ç—‡çŠ¶ä¸ç–¾ç—…çš„æ˜ å°„å…³ç³»
        self.symptom_disease_mapping = {
            "å¤´ç—›å¤´æ™•": ["é«˜è¡€å‹", "è„‘è¡€ç®¡ç—…", "ç¥ç»è¡°å¼±"],
            "å’³å—½å’³ç—°": ["æ…¢æ€§æ°”ç®¡ç‚", "è‚ºç»“æ ¸ç—…", "å’½ç‚"],
            "èƒ¸é—·å¿ƒæ‚¸": ["å† å¿ƒç—…", "ç¥ç»è¡°å¼±", "ç”²äº¢"],
            "è…¹ç—›è…¹æ³»": ["è‚ ç‚", "æ¶ˆåŒ–æ€§æºƒç–¡", "è‚ç¡¬åŒ–"],
            "è…°ç—›": ["è‚¾ç‚", "è…°æ¤ç—…", "å‰åˆ—è…ºç‚"],
            "æœˆç»å¼‚å¸¸": ["æœˆç»å¤±è°ƒ", "å­å®«è„±å‚", "æµäº§"],
            "å¤±çœ å¤šæ¢¦": ["ç¥ç»è¡°å¼±", "æŠ‘éƒç—‡", "ç„¦è™‘ç—‡"],
            "ä¹åŠ›è™šå¼±": ["è™šæŸ", "è€å¹´ç—…", "ç”²å‡"]
        }
        
        # ç—‡çŠ¶æƒé‡ï¼ˆæ ¹æ®ä¸­åŒ»é‡è¦æ€§ï¼‰
        self.symptom_weights = {
            "ä¸»è¯‰": 1.5,
            "å‘çƒ­": 1.3,
            "ç–¼ç—›": 1.3,
            "å’³å—½": 1.2,
            "å¤±çœ ": 1.2,
            "ä¾¿ç§˜": 1.1,
            "è…¹æ³»": 1.1
        }
        
        self.load_knowledge_base()
        
    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        try:
            if os.path.exists(self.faiss_index_file):
                self.faiss_index = faiss.read_index(self.faiss_index_file)
                
            with open(self.documents_file, 'rb') as f:
                self.documents = pickle.load(f)
                
            with open(self.metadata_file, 'rb') as f:
                self.metadata = pickle.load(f)
                
            # å»ºç«‹TF-IDFç´¢å¼•ç”¨äºå…³é”®è¯æ£€ç´¢
            self._build_tfidf_index()
            
        except Exception as e:
            print(f"Error loading knowledge base: {e}")
            
    def _build_tfidf_index(self):
        """å»ºç«‹TF-IDFç´¢å¼•"""
        if not self.documents:
            return
            
        # ä¸­æ–‡åˆ†è¯
        segmented_docs = []
        for doc in self.documents:
            # ä½¿ç”¨jiebaåˆ†è¯ï¼Œä¿ç•™ä¸­åŒ»ä¸“ä¸šè¯æ±‡
            words = jieba.cut(doc)
            segmented_docs.append(" ".join(words))
            
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            stop_words=self._get_chinese_stopwords()
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(segmented_docs)
        
    def _get_chinese_stopwords(self) -> List[str]:
        """ä¸­æ–‡åœç”¨è¯"""
        return ["çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰", "å’Œ", "ä¸", "åŠ", "æˆ–", "ä½†", "è€Œ", "å› ä¸º", "æ‰€ä»¥"]
        
    def expand_query(self, query: str) -> str:
        """æŸ¥è¯¢æ‰©å±• - æ·»åŠ åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­"""
        expanded_terms = [query]
        
        # 1. ç›´æ¥åŒä¹‰è¯æ‰©å±•
        for main_term, synonyms in self.tcm_synonyms.items():
            if main_term in query:
                expanded_terms.extend(synonyms[:3])  # æ·»åŠ å‰3ä¸ªæœ€ç›¸å…³çš„åŒä¹‰è¯
            for synonym in synonyms:
                if synonym in query and main_term not in expanded_terms:
                    expanded_terms.append(main_term)
                    break
        
        # 2. ç—‡çŠ¶-ç–¾ç—…å…³è”æ‰©å±•
        for symptom_key, diseases in self.symptom_disease_mapping.items():
            if any(term in query for term in symptom_key.split()):
                expanded_terms.extend(diseases[:2])  # æ·»åŠ ç›¸å…³ç–¾ç—…
                
        # å»é‡å¹¶è¿”å›ï¼ˆä¿æŒé¡ºåºï¼‰
        unique_terms = list(dict.fromkeys(expanded_terms))
        return " ".join(unique_terms)
        
    def semantic_search(self, query_embedding: List[float], k: int = 10) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢"""
        if self.faiss_index.ntotal == 0:
            return []
            
        query_array = np.array([query_embedding], dtype=np.float32)
        faiss.normalize_L2(query_array)
        
        scores, indices = self.faiss_index.search(query_array, min(k, self.faiss_index.ntotal))
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx >= 0 and idx < len(self.documents):
                results.append({
                    'document': self.documents[idx],
                    'metadata': self.metadata[idx],
                    'semantic_score': float(scores[0][i]),
                    'index': idx,
                    'method': 'semantic'
                })
                
        # ç¡®ä¿å³ä½¿æ²¡æœ‰é«˜åˆ†ç»“æœä¹Ÿè¿”å›æœ€ä½³çš„å‡ ä¸ª
        if len(results) == 0 and len(top_indices) > 0:
            for idx in top_indices[:k]:
                results.append({
                    'document': self.documents[idx],
                    'metadata': self.metadata[idx],
                    'similarity_score': float(similarities[idx]),
                    'keyword_score': float(similarities[idx]),
                    'source': self.metadata[idx].get('source', 'unknown'),
                    'index': idx,
                    'method': 'keyword'
                })
        
        return results
        
    def keyword_search(self, query: str, k: int = 10) -> List[Dict]:
        """å…³é”®è¯æ£€ç´¢ï¼ˆTF-IDFï¼‰"""
        if self.tfidf_matrix is None:
            return []
            
        # æ‰©å±•æŸ¥è¯¢
        expanded_query = self.expand_query(query)
        
        # åˆ†è¯å¹¶å‘é‡åŒ–æŸ¥è¯¢
        words = jieba.cut(expanded_query)
        query_text = " ".join(words)
        query_vector = self.tfidf_vectorizer.transform([query_text])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # è·å–top-kç»“æœ
        top_indices = similarities.argsort()[-k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.001:  # é™ä½é˜ˆå€¼ç¡®ä¿æ›´å¤šç»“æœ
                results.append({
                    'document': self.documents[idx],
                    'metadata': self.metadata[idx],
                    'similarity_score': float(similarities[idx]),  # ç»Ÿä¸€å­—æ®µå
                    'keyword_score': float(similarities[idx]),     # ä¿æŒå…¼å®¹æ€§
                    'source': self.metadata[idx].get('source', 'unknown'),
                    'index': idx,
                    'method': 'keyword'
                })
                
        # ç¡®ä¿å³ä½¿æ²¡æœ‰é«˜åˆ†ç»“æœä¹Ÿè¿”å›æœ€ä½³çš„å‡ ä¸ª
        if len(results) == 0 and len(top_indices) > 0:
            for idx in top_indices[:k]:
                results.append({
                    'document': self.documents[idx],
                    'metadata': self.metadata[idx],
                    'similarity_score': float(similarities[idx]),
                    'keyword_score': float(similarities[idx]),
                    'source': self.metadata[idx].get('source', 'unknown'),
                    'index': idx,
                    'method': 'keyword'
                })
        
        return results
        
    def hybrid_search(self, query: str, query_embedding: List[float], 
                     semantic_weight: float = 0.7, keyword_weight: float = 0.3,
                     total_results: int = 5) -> List[Dict]:
        """æ··åˆæ£€ç´¢ - èåˆè¯­ä¹‰å’Œå…³é”®è¯æ£€ç´¢ç»“æœ"""
        
        # åˆ†åˆ«è¿›è¡Œä¸¤ç§æ£€ç´¢
        semantic_results = self.semantic_search(query_embedding, k=15)
        keyword_results = self.keyword_search(query, k=15)
        
        # åˆ›å»ºæ–‡æ¡£ç´¢å¼•åˆ°ç»“æœçš„æ˜ å°„
        result_map = defaultdict(dict)
        
        # å¤„ç†è¯­ä¹‰æ£€ç´¢ç»“æœ
        for result in semantic_results:
            idx = result['index']
            result_map[idx]['document'] = result['document']
            result_map[idx]['metadata'] = result['metadata']
            result_map[idx]['semantic_score'] = result['semantic_score']
            
        # å¤„ç†å…³é”®è¯æ£€ç´¢ç»“æœ
        for result in keyword_results:
            idx = result['index']
            if idx not in result_map:
                result_map[idx]['document'] = result['document']
                result_map[idx]['metadata'] = result['metadata']
            result_map[idx]['keyword_score'] = result['keyword_score']
            
        # è®¡ç®—æ··åˆåˆ†æ•°
        final_results = []
        for idx, data in result_map.items():
            semantic_score = data.get('semantic_score', 0.0)
            keyword_score = data.get('keyword_score', 0.0)
            
            # å½’ä¸€åŒ–åˆ†æ•°
            normalized_semantic = self._normalize_score(semantic_score, 'semantic')
            normalized_keyword = self._normalize_score(keyword_score, 'keyword')
            
            # è®¡ç®—ç—‡çŠ¶æƒé‡åŠ æˆ
            symptom_bonus = self._calculate_symptom_bonus(query, data['document'])
            
            # è®¡ç®—ç–¾ç—…ç²¾ç¡®åŒ¹é…åŠ æˆ
            disease_exact_bonus = self._calculate_disease_exact_bonus(query, data['document'], data['metadata'])
            
            # æ··åˆåˆ†æ•°
            hybrid_score = (semantic_weight * normalized_semantic + 
                          keyword_weight * normalized_keyword + 
                          symptom_bonus + disease_exact_bonus)
            
            final_results.append({
                'document': data['document'],
                'metadata': data['metadata'],
                'hybrid_score': hybrid_score,
                'semantic_score': semantic_score,
                'keyword_score': keyword_score,
                'symptom_bonus': symptom_bonus,
                'disease_exact_bonus': disease_exact_bonus
            })
            
        # æŒ‰æ··åˆåˆ†æ•°æ’åºå¹¶å»é‡
        final_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        final_results = self._remove_duplicates(final_results)
        
        return final_results[:total_results]
        
    def _normalize_score(self, score: float, method: str) -> float:
        """åˆ†æ•°å½’ä¸€åŒ–"""
        if method == 'semantic':
            # FAISSå†…ç§¯åˆ†æ•°é€šå¸¸åœ¨0-1ä¹‹é—´
            return max(0, min(1, score))
        elif method == 'keyword':
            # TF-IDFä½™å¼¦ç›¸ä¼¼åº¦åœ¨0-1ä¹‹é—´
            return score
        return score
        
    def _calculate_symptom_bonus(self, query: str, document: str) -> float:
        """è®¡ç®—ç—‡çŠ¶æƒé‡åŠ æˆ"""
        bonus = 0.0
        query_lower = query.lower()
        doc_lower = document.lower()
        
        for symptom, weight in self.symptom_weights.items():
            if symptom in query_lower and symptom in doc_lower:
                bonus += (weight - 1.0) * 0.1  # è½¬æ¢ä¸º0.1ä»¥å†…çš„åŠ æˆ
                
        return min(bonus, 0.3)  # æœ€å¤§åŠ æˆ0.3
    
    def _calculate_disease_exact_bonus(self, query: str, document: str, metadata: Dict) -> float:
        """è®¡ç®—ç–¾ç—…ç²¾ç¡®åŒ¹é…åŠ æˆ - æ ¸å¿ƒåŠŸèƒ½"""
        bonus = 0.0
        query_clean = query.strip().lower()
        doc_lower = document.lower()
        source = metadata.get('source', '').lower()
        
        # 1. ç›´æ¥ç–¾ç—…ååŒ¹é… - æœ€é«˜ä¼˜å…ˆçº§
        for disease_name, weight in self.disease_exact_weights.items():
            disease_lower = disease_name.lower()
            
            # ç²¾ç¡®åŒ¹é…æŸ¥è¯¢
            if disease_lower == query_clean:
                # æ–‡æ¡£æ ‡é¢˜åŒ¹é…
                if disease_lower in source:
                    bonus += weight * 0.8  # è¶…é«˜æƒé‡ï¼šæ ‡é¢˜ç²¾ç¡®åŒ¹é…
                    print(f"ğŸ¯ ç–¾ç—…ç²¾ç¡®åŒ¹é…(æ ‡é¢˜): {disease_name} -> æƒé‡:{weight * 0.8:.1f}")
                # æ–‡æ¡£å†…å®¹åŒ¹é…
                elif disease_lower in doc_lower:
                    bonus += weight * 0.6  # é«˜æƒé‡ï¼šå†…å®¹ç²¾ç¡®åŒ¹é…
                    print(f"ğŸ¯ ç–¾ç—…ç²¾ç¡®åŒ¹é…(å†…å®¹): {disease_name} -> æƒé‡:{weight * 0.6:.1f}")
                    
        # 2. ç–¾ç—…ååŒ…å«åŒ¹é… - æ¬¡ä¼˜å…ˆçº§
        for disease_name, weight in self.disease_exact_weights.items():
            disease_lower = disease_name.lower()
            
            # æŸ¥è¯¢åŒ…å«ç–¾ç—…å
            if disease_lower in query_clean and len(disease_lower) >= 3:
                if disease_lower in source:
                    bonus += weight * 0.5  # ä¸­é«˜æƒé‡ï¼šæ ‡é¢˜éƒ¨åˆ†åŒ¹é…
                elif disease_lower in doc_lower:
                    bonus += weight * 0.3  # ä¸­ç­‰æƒé‡ï¼šå†…å®¹éƒ¨åˆ†åŒ¹é…
                    
        # 3. åŒä¹‰è¯ç²¾ç¡®åŒ¹é… - ç¬¬ä¸‰ä¼˜å…ˆçº§
        for main_disease, synonyms in self.tcm_synonyms.items():
            if main_disease in self.disease_exact_weights:
                main_weight = self.disease_exact_weights[main_disease]
                
                for synonym in synonyms:
                    synonym_lower = synonym.lower()
                    
                    # åŒä¹‰è¯ç²¾ç¡®åŒ¹é…
                    if synonym_lower == query_clean:
                        if synonym_lower in source or main_disease.lower() in source:
                            bonus += main_weight * 0.4  # åŒä¹‰è¯æ ‡é¢˜åŒ¹é…
                        elif synonym_lower in doc_lower:
                            bonus += main_weight * 0.25  # åŒä¹‰è¯å†…å®¹åŒ¹é…
                            
        # 4. é™åˆ¶æœ€å¤§åŠ æˆï¼Œé¿å…åˆ†æ•°çˆ†ç‚¸
        return min(bonus, 15.0)  # æœ€å¤§åŠ æˆ15.0
        
    def _remove_duplicates(self, results: List[Dict], threshold: float = 0.85) -> List[Dict]:
        """å»é™¤é‡å¤å’Œé«˜åº¦ç›¸ä¼¼çš„ç»“æœ"""
        if len(results) <= 1:
            return results
            
        filtered_results = [results[0]]  # ä¿ç•™ç¬¬ä¸€ä¸ªç»“æœ
        
        for current in results[1:]:
            is_duplicate = False
            current_doc = current['document']
            
            for existing in filtered_results:
                existing_doc = existing['document']
                
                # è®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦
                similarity = self._calculate_document_similarity(current_doc, existing_doc)
                
                if similarity > threshold:
                    is_duplicate = True
                    # å¦‚æœå½“å‰ç»“æœåˆ†æ•°æ›´é«˜ï¼Œæ›¿æ¢ç°æœ‰ç»“æœ
                    if current['hybrid_score'] > existing['hybrid_score']:
                        filtered_results.remove(existing)
                        filtered_results.append(current)
                    break
                    
            if not is_duplicate:
                filtered_results.append(current)
                
        return filtered_results
        
    def _calculate_document_similarity(self, doc1: str, doc2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦"""
        # ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—
        if len(doc1) == 0 or len(doc2) == 0:
            return 0.0
            
        # è®¡ç®—å­—ç¬¦é‡å åº¦
        set1 = set(doc1)
        set2 = set(doc2)
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        if union == 0:
            return 0.0
            
        return intersection / union