#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TCMç³»ç»Ÿé«˜å¹¶å‘å‹åŠ›æµ‹è¯•å·¥å…·
ç”¨äºéªŒè¯ç³»ç»Ÿåœ¨æ¯”èµ›æœŸé—´çš„é«˜å¹¶å‘æ€§èƒ½è¡¨ç°
"""

import asyncio
import aiohttp
import time
import json
from concurrent.futures import ThreadPoolExecutor
import requests
from datetime import datetime

# æµ‹è¯•é…ç½®
BASE_URL = "http://127.0.0.1:8000"
TEST_SCENARIOS = [
    {
        "name": "åŸºç¡€æµ‹è¯•",
        "concurrent_users": 2,
        "total_requests": 4,
        "description": "éªŒè¯åŸºæœ¬åŠŸèƒ½"
    },
    {
        "name": "è½»åº¦å¹¶å‘æµ‹è¯•",
        "concurrent_users": 5,
        "total_requests": 10,
        "description": "æ¨¡æ‹Ÿæ­£å¸¸ä½¿ç”¨åœºæ™¯"
    },
    {
        "name": "ä¸­åº¦å¹¶å‘æµ‹è¯•", 
        "concurrent_users": 10,
        "total_requests": 20,
        "description": "æ¨¡æ‹Ÿè¯„å§”åŒæ—¶æµ‹è¯•"
    }
]

def test_basic_endpoints():
    """æµ‹è¯•åŸºç¡€ç«¯ç‚¹å“åº”"""
    print("ğŸ” æµ‹è¯•åŸºç¡€ç«¯ç‚¹...")
    
    endpoints = [
        "/debug_status",
        "/static/index_v2.html",
        "/doctor/thinking-v2"
    ]
    
    results = {}
    for endpoint in endpoints:
        try:
            start_time = time.time()
            response = requests.get(f"{BASE_URL}{endpoint}", timeout=10)
            response_time = time.time() - start_time
            
            results[endpoint] = {
                "status": response.status_code,
                "response_time": f"{response_time:.3f}s",
                "size": len(response.content)
            }
            
            status_icon = "âœ…" if response.status_code == 200 else "âŒ"
            print(f"  {status_icon} {endpoint}: {response.status_code} ({response_time:.3f}s)")
            
        except Exception as e:
            results[endpoint] = {"error": str(e)}
            print(f"  âŒ {endpoint}: {str(e)}")
    
    return results

async def send_chat_request(session, user_id, test_query="å¤´ç—›ï¼Œç¡çœ ä¸å¥½"):
    """å‘é€èŠå¤©è¯·æ±‚"""
    
    payload = {
        "message": test_query,
        "conversation_id": f"test_concurrent_{user_id}_{int(time.time())}",
        "selected_doctor": "zhang_zhongjing"
    }
    
    try:
        start_time = time.time()
        async with session.post(
            f"{BASE_URL}/chat_with_ai",
            json=payload,
            timeout=aiohttp.ClientTimeout(total=60)
        ) as response:
            response_time = time.time() - start_time
            
            if response.status == 200:
                data = await response.json()
                return {
                    "success": True,
                    "user_id": user_id,
                    "response_time": response_time,
                    "response_length": len(data.get("response", "")),
                    "has_cache_hit": "cache" in data.get("response", "").lower()
                }
            else:
                return {
                    "success": False,
                    "user_id": user_id,
                    "error": f"HTTP {response.status}",
                    "response_time": response_time
                }
                
    except asyncio.TimeoutError:
        return {
            "success": False,
            "user_id": user_id,
            "error": "Timeout",
            "response_time": 60.0
        }
    except Exception as e:
        return {
            "success": False,
            "user_id": user_id,
            "error": str(e),
            "response_time": time.time() - start_time
        }

async def run_concurrent_test(concurrent_users, total_requests):
    """è¿è¡Œå¹¶å‘æµ‹è¯•"""
    
    connector = aiohttp.TCPConnector(limit=100, limit_per_host=50)
    timeout = aiohttp.ClientTimeout(total=60)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for i in range(total_requests):
            user_id = i % concurrent_users
            task = send_chat_request(session, user_id)
            tasks.append(task)
        
        # åˆ†æ‰¹æ‰§è¡Œï¼Œé¿å…è¿‡å¤šå¹¶å‘
        batch_size = concurrent_users
        results = []
        
        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i + batch_size]
            print(f"  æ‰§è¡Œæ‰¹æ¬¡ {i//batch_size + 1}/{(len(tasks)-1)//batch_size + 1}...")
            
            batch_results = await asyncio.gather(*batch, return_exceptions=True)
            results.extend(batch_results)
            
            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…å‹å®ç³»ç»Ÿ
            await asyncio.sleep(1)
    
    return results

def analyze_results(results, test_name):
    """åˆ†ææµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ“Š {test_name} - ç»“æœåˆ†æ:")
    
    successful = [r for r in results if isinstance(r, dict) and r.get("success")]
    failed = [r for r in results if isinstance(r, dict) and not r.get("success")]
    exceptions = [r for r in results if not isinstance(r, dict)]
    
    total = len(results)
    success_rate = len(successful) / total * 100 if total > 0 else 0
    
    print(f"  ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}% ({len(successful)}/{total})")
    print(f"  âŒ å¤±è´¥æ•°: {len(failed)}")
    print(f"  ğŸ’¥ å¼‚å¸¸æ•°: {len(exceptions)}")
    
    if successful:
        response_times = [r["response_time"] for r in successful]
        avg_time = sum(response_times) / len(response_times)
        max_time = max(response_times)
        min_time = min(response_times)
        
        print(f"  â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}s")
        print(f"  ğŸš€ æœ€å¿«å“åº”: {min_time:.2f}s")
        print(f"  ğŸŒ æœ€æ…¢å“åº”: {max_time:.2f}s")
        
        # ç¼“å­˜å‘½ä¸­åˆ†æ
        cache_hits = len([r for r in successful if r.get("has_cache_hit")])
        cache_rate = cache_hits / len(successful) * 100
        print(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {cache_rate:.1f}%")
    
    if failed:
        error_types = {}
        for r in failed:
            error = r.get("error", "Unknown")
            error_types[error] = error_types.get(error, 0) + 1
        
        print(f"  ğŸ” é”™è¯¯ç±»å‹:")
        for error, count in error_types.items():
            print(f"    - {error}: {count}æ¬¡")
    
    # æ€§èƒ½è¯„çº§
    if success_rate >= 95 and successful:
        avg_time = sum(r["response_time"] for r in successful) / len(successful)
        if avg_time <= 10:
            grade = "ğŸ† ä¼˜ç§€"
        elif avg_time <= 20:
            grade = "ğŸ‘ è‰¯å¥½"
        else:
            grade = "âš ï¸ éœ€ä¼˜åŒ–"
    else:
        grade = "âŒ éœ€ä¿®å¤"
    
    print(f"  ğŸ“ æ€§èƒ½è¯„çº§: {grade}")
    
    return {
        "success_rate": success_rate,
        "total_requests": total,
        "successful_requests": len(successful),
        "failed_requests": len(failed),
        "avg_response_time": sum(r["response_time"] for r in successful) / len(successful) if successful else 0,
        "grade": grade
    }

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    print("ğŸ¥ TCMç³»ç»Ÿé«˜å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç›®æ ‡åœ°å€: {BASE_URL}")
    print()
    
    # 1. åŸºç¡€ç«¯ç‚¹æµ‹è¯•
    basic_results = test_basic_endpoints()
    print()
    
    # 2. é«˜å¹¶å‘æµ‹è¯•
    all_results = {}
    
    for scenario in TEST_SCENARIOS:
        print(f"ğŸš€ å¼€å§‹ {scenario['name']}")
        print(f"   {scenario['description']}")
        print(f"   å¹¶å‘ç”¨æˆ·: {scenario['concurrent_users']}")
        print(f"   æ€»è¯·æ±‚æ•°: {scenario['total_requests']}")
        
        try:
            start_time = time.time()
            results = asyncio.run(run_concurrent_test(
                scenario['concurrent_users'], 
                scenario['total_requests']
            ))
            total_time = time.time() - start_time
            
            analysis = analyze_results(results, scenario['name'])
            analysis['total_test_time'] = total_time
            analysis['qps'] = scenario['total_requests'] / total_time
            
            all_results[scenario['name']] = analysis
            
            print(f"  ğŸ• æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}s")
            print(f"  ğŸ“Š å¹³å‡QPS: {analysis['qps']:.2f} req/s")
            
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            all_results[scenario['name']] = {"error": str(e)}
        
        print("-" * 40)
    
    # 3. æ€»ç»“æŠ¥å‘Š
    print("\nğŸ¯ é«˜å¹¶å‘æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 50)
    
    for test_name, result in all_results.items():
        if "error" not in result:
            print(f"{test_name}:")
            print(f"  æˆåŠŸç‡: {result['success_rate']:.1f}%")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']:.2f}s")
            print(f"  QPS: {result['qps']:.2f}")
            print(f"  è¯„çº§: {result['grade']}")
        else:
            print(f"{test_name}: âŒ {result['error']}")
        print()
    
    # 4. æ¯”èµ›å‡†å¤‡å»ºè®®
    print("ğŸ“‹ æ¯”èµ›å‡†å¤‡å»ºè®®:")
    
    high_test = all_results.get("é«˜å¹¶å‘å‹åŠ›æµ‹è¯•", {})
    if high_test.get("success_rate", 0) >= 90:
        print("  âœ… ç³»ç»Ÿé«˜å¹¶å‘æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥åº”å¯¹æ¯”èµ›ç°åœºè®¿é—®")
    else:
        print("  âš ï¸ å»ºè®®ä¼˜åŒ–é«˜å¹¶å‘æ€§èƒ½")
    
    if any(r.get("avg_response_time", 0) > 15 for r in all_results.values() if "error" not in r):
        print("  ğŸ’¡ å»ºè®®å¯ç”¨æ›´å¤šç¼“å­˜æˆ–å¢åŠ æœåŠ¡å™¨èµ„æº")
    
    print("  ğŸ”§ å»ºè®®æ¯”èµ›å‰:")
    print("    - é‡å¯æœåŠ¡æ¸…ç†å†…å­˜")
    print("    - é¢„çƒ­ç¼“å­˜ç³»ç»Ÿ")
    print("    - ç›‘æ§æœåŠ¡å™¨èµ„æºä½¿ç”¨")
    
    # ä¿å­˜ç»“æœ
    with open('/opt/tcm/concurrency_test_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'test_time': datetime.now().isoformat(),
            'basic_endpoints': basic_results,
            'concurrency_tests': all_results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: /opt/tcm/concurrency_test_results.json")

if __name__ == "__main__":
    main()