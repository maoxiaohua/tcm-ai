#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMé›†æˆå¤„ç†æ¨¡å—
ä»chat_with_ai_endpointå‡½æ•°ä¸­æå–çš„AIå¤§æ¨¡å‹é›†æˆç›¸å…³åŠŸèƒ½
"""

import logging
from typing import Dict, Any, Optional, List
from datetime import datetime

from ..utils.common_utils import safe_execute, get_current_timestamp_iso
from ..utils.text_utils import clean_ai_response_text, split_long_text, count_chinese_characters
from ...core.prescription.prescription_format_enforcer import get_prescription_enforcer

logger = logging.getLogger(__name__)

class LLMIntegrationProcessor:
    """LLMé›†æˆå¤„ç†å™¨"""
    
    def __init__(self, llm_service, config_manager):
        self.llm_service = llm_service
        self.config = config_manager.get_ai_config()
        
        # LLMé…ç½®å‚æ•°
        self.default_model = self.config.get('default_model', 'qwen-max')
        self.default_temperature = self.config.get('temperature', 0.7)
        self.max_tokens = self.config.get('max_tokens', 2000)
        self.timeout = self.config.get('timeout', 60)
        
        # æç¤ºè¯æ¨¡æ¿
        self.prompt_templates = {
            'tcm_diagnosis': self._get_tcm_diagnosis_template(),
            'prescription_generation': self._get_prescription_template(),
            'symptom_analysis': self._get_symptom_analysis_template(),
            'lifestyle_advice': self._get_lifestyle_advice_template(),
            'follow_up': self._get_follow_up_template()
        }
        
        # å“åº”è´¨é‡æ£€æŸ¥è§„åˆ™
        self.quality_rules = {
            'min_length': 100,
            'max_length': 3000,
            'required_sections': ['åˆ†æ', 'å»ºè®®'],
            'forbidden_words': ['ä¸è´Ÿè´£ä»»', 'ä»…ä¾›å‚è€ƒ', 'è¯·å’¨è¯¢åŒ»ç”Ÿ']
        }
    
    def generate_ai_response(self, query: str, context: Dict[str, Any], 
                           response_type: str = 'tcm_diagnosis') -> Dict[str, Any]:
        """ç”ŸæˆAIå“åº”"""
        response_result = {
            'success': False,
            'response': '',
            'response_type': response_type,
            'metadata': {},
            'quality_score': 0.0,
            'processing_time': 0,
            'error_message': ''
        }
        
        start_time = datetime.now()
        
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(query, context, response_type)
            
            # å‚æ•°é…ç½®
            llm_params = self._get_llm_parameters(response_type, context)
            
            # è°ƒç”¨LLMæœåŠ¡
            llm_response = self._call_llm_service(prompt, llm_params)
            
            if llm_response['success']:
                # åå¤„ç†å“åº”
                processed_response = self._post_process_response(
                    llm_response['response'], 
                    response_type
                )
                
                # è´¨é‡è¯„ä¼°
                quality_score = self._evaluate_response_quality(
                    processed_response, 
                    query, 
                    response_type
                )
                
                processing_time = (datetime.now() - start_time).total_seconds()
                
                response_result.update({
                    'success': True,
                    'response': processed_response,
                    'metadata': {
                        'model_used': llm_params.get('model', self.default_model),
                        'temperature': llm_params.get('temperature', self.default_temperature),
                        'prompt_tokens': len(prompt),
                        'response_tokens': len(processed_response),
                        'chinese_chars': count_chinese_characters(processed_response)
                    },
                    'quality_score': quality_score,
                    'processing_time': processing_time
                })
                
                # è´¨é‡æ£€æŸ¥
                if quality_score < 0.6:
                    logger.warning(f"å“åº”è´¨é‡è¾ƒä½: {quality_score}")
                    # å¯ä»¥é€‰æ‹©é‡æ–°ç”Ÿæˆæˆ–æ·»åŠ è­¦å‘Šä¿¡æ¯
                
            else:
                response_result['error_message'] = llm_response.get('error', 'LLMè°ƒç”¨å¤±è´¥')
                
        except Exception as e:
            logger.error(f"AIå“åº”ç”Ÿæˆå¤±è´¥: {e}")
            response_result['error_message'] = str(e)
            response_result['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        return response_result
    
    def _build_prompt(self, query: str, context: Dict[str, Any], 
                     response_type: str) -> str:
        """æ„å»ºæç¤ºè¯"""
        # è·å–åŸºç¡€æ¨¡æ¿
        base_template = self.prompt_templates.get(response_type, self.prompt_templates['tcm_diagnosis'])
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = self._build_context_info(context)
        
        # ç”¨æˆ·æŸ¥è¯¢é¢„å¤„ç†
        cleaned_query = clean_ai_response_text(query)
        
        # ç»„è£…å®Œæ•´æç¤ºè¯
        full_prompt = f"""
{base_template}

ç”¨æˆ·å’¨è¯¢å†…å®¹ï¼š
{cleaned_query}

{context_info}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®ã€å®ç”¨çš„ä¸­åŒ»è¯Šç–—å»ºè®®ã€‚
"""
        
        return full_prompt.strip()
    
    def _build_context_info(self, context: Dict[str, Any]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []
        
        # çŸ¥è¯†æ£€ç´¢ä¸Šä¸‹æ–‡
        if context.get('retrieved_knowledge'):
            knowledge_items = context['retrieved_knowledge'].get('retrieved_items', [])
            if knowledge_items:
                context_parts.append("ç›¸å…³çŸ¥è¯†å‚è€ƒï¼š")
                for i, item in enumerate(knowledge_items[:3], 1):
                    content = item.get('content', '')[:300]
                    context_parts.append(f"{i}. {content}")
        
        # ç”¨æˆ·å†å²ä¸Šä¸‹æ–‡
        if context.get('user_history'):
            history_summary = context['user_history'].get('context_summary', '')
            if history_summary:
                context_parts.append(f"\nç”¨æˆ·èƒŒæ™¯ï¼š{history_summary}")
        
        # å›¾åƒåˆ†æä¸Šä¸‹æ–‡
        if context.get('image_analysis'):
            for analysis in context['image_analysis']:
                if analysis.get('success'):
                    features = analysis.get('features_extracted', {})
                    if features:
                        context_parts.append("\nå›¾åƒåˆ†æç»“æœï¼š")
                        for feature, description in features.items():
                            context_parts.append(f"- {feature}: {description}")
        
        # ç—‡çŠ¶ä¸Šä¸‹æ–‡
        if context.get('symptoms'):
            symptoms_text = 'ã€'.join(context['symptoms'])
            context_parts.append(f"\nä¸»è¦ç—‡çŠ¶ï¼š{symptoms_text}")
        
        return '\n'.join(context_parts) if context_parts else ""
    
    def _get_llm_parameters(self, response_type: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–LLMå‚æ•°é…ç½®"""
        base_params = {
            'model': self.default_model,
            'temperature': self.default_temperature,
            'max_tokens': self.max_tokens,
            'timeout': self.timeout
        }
        
        # æ ¹æ®å“åº”ç±»å‹è°ƒæ•´å‚æ•°
        type_adjustments = {
            'tcm_diagnosis': {'temperature': 0.7, 'max_tokens': 2000},
            'prescription_generation': {'temperature': 0.5, 'max_tokens': 1500},
            'symptom_analysis': {'temperature': 0.8, 'max_tokens': 1200},
            'lifestyle_advice': {'temperature': 0.9, 'max_tokens': 1000},
            'follow_up': {'temperature': 0.6, 'max_tokens': 800}
        }
        
        if response_type in type_adjustments:
            base_params.update(type_adjustments[response_type])
        
        # æ ¹æ®ä¸Šä¸‹æ–‡å¤æ‚åº¦è°ƒæ•´
        context_complexity = self._assess_context_complexity(context)
        if context_complexity > 0.7:
            base_params['max_tokens'] = min(base_params['max_tokens'] + 500, 3000)
            base_params['temperature'] = max(base_params['temperature'] - 0.1, 0.3)
        
        return base_params
    
    def _assess_context_complexity(self, context: Dict[str, Any]) -> float:
        """è¯„ä¼°ä¸Šä¸‹æ–‡å¤æ‚åº¦"""
        complexity_score = 0.0
        
        # çŸ¥è¯†æ£€ç´¢å¤æ‚åº¦
        if context.get('retrieved_knowledge', {}).get('retrieved_items'):
            knowledge_count = len(context['retrieved_knowledge']['retrieved_items'])
            complexity_score += min(knowledge_count * 0.1, 0.3)
        
        # å›¾åƒåˆ†æå¤æ‚åº¦
        if context.get('image_analysis'):
            image_count = len(context['image_analysis'])
            complexity_score += min(image_count * 0.15, 0.3)
        
        # ç”¨æˆ·å†å²å¤æ‚åº¦
        if context.get('user_history', {}).get('recent_history'):
            history_count = len(context['user_history']['recent_history'])
            complexity_score += min(history_count * 0.1, 0.2)
        
        # ç—‡çŠ¶æ•°é‡å¤æ‚åº¦
        if context.get('symptoms'):
            symptom_count = len(context['symptoms'])
            complexity_score += min(symptom_count * 0.05, 0.2)
        
        return min(complexity_score, 1.0)
    
    def _call_llm_service(self, prompt: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨LLMæœåŠ¡"""
        try:
            response = self.llm_service.generate_response(
                prompt=prompt,
                model=params.get('model', self.default_model),
                temperature=params.get('temperature', self.default_temperature),
                max_tokens=params.get('max_tokens', self.max_tokens),
                timeout=params.get('timeout', self.timeout)
            )
            
            return response
            
        except Exception as e:
            logger.error(f"LLMæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _post_process_response(self, response: str, response_type: str) -> str:
        """åå¤„ç†AIå“åº”"""
        # åŸºç¡€æ¸…ç†
        cleaned_response = clean_ai_response_text(response)
        
        # ğŸ”¥ å¤„æ–¹æ ¼å¼å¼ºåˆ¶æ‰§è¡Œ (v2.9æ–°å¢) - ç¡®ä¿AIå›å¤åŒ…å«å…·ä½“å‰‚é‡
        try:
            prescription_enforcer = get_prescription_enforcer()
            cleaned_response = prescription_enforcer.enforce_prescription_format(cleaned_response)
            logger.info("å¤„æ–¹æ ¼å¼å¼ºåˆ¶æ‰§è¡Œå®Œæˆ")
        except Exception as e:
            logger.error(f"å¤„æ–¹æ ¼å¼å¼ºåˆ¶æ‰§è¡Œå¤±è´¥: {e}")
        
        # æ ¹æ®ç±»å‹è¿›è¡Œç‰¹å®šå¤„ç†
        if response_type == 'prescription_generation':
            cleaned_response = self._format_prescription_response(cleaned_response)
        elif response_type == 'symptom_analysis':
            cleaned_response = self._format_symptom_analysis_response(cleaned_response)
        elif response_type == 'lifestyle_advice':
            cleaned_response = self._format_lifestyle_advice_response(cleaned_response)
        
        # é•¿åº¦æ§åˆ¶
        if len(cleaned_response) > 3000:
            cleaned_response = self._truncate_response_intelligently(cleaned_response)
        
        # æ·»åŠ å…è´£å£°æ˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.config.get('add_disclaimer', True):
            cleaned_response = self._add_medical_disclaimer(cleaned_response)
        
        return cleaned_response
    
    def _format_prescription_response(self, response: str) -> str:
        """æ ¼å¼åŒ–å¤„æ–¹å“åº”"""
        # ç¡®ä¿å¤„æ–¹æ ¼å¼æ­£ç¡®
        lines = response.split('\n')
        formatted_lines = []
        
        in_prescription_section = False
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # è¯†åˆ«å¤„æ–¹éƒ¨åˆ†
            if 'å¤„æ–¹' in line or 'è¯æ–¹' in line:
                in_prescription_section = True
                formatted_lines.append(line)
                continue
            
            # æ ¼å¼åŒ–å¤„æ–¹æ¡ç›®
            if in_prescription_section and any(char in line for char in ['g', 'å…‹', 'é’±']):
                # ç¡®ä¿è¯æåå’Œç”¨é‡æ ¼å¼æ­£ç¡®
                from ..utils.text_utils import normalize_prescription_dosage
                formatted_line = normalize_prescription_dosage(line)
                formatted_lines.append(formatted_line)
            else:
                formatted_lines.append(line)
                if line.endswith('ã€‚') or line.endswith('ï¼š'):
                    in_prescription_section = False
        
        return '\n'.join(formatted_lines)
    
    def _format_symptom_analysis_response(self, response: str) -> str:
        """æ ¼å¼åŒ–ç—‡çŠ¶åˆ†æå“åº”"""
        # ç¡®ä¿æœ‰æ˜ç¡®çš„åˆ†æç»“æ„
        if 'ç—‡çŠ¶åˆ†æ' not in response and 'åˆ†æ' not in response:
            response = f"## ç—‡çŠ¶åˆ†æ\n\n{response}"
        
        return response
    
    def _format_lifestyle_advice_response(self, response: str) -> str:
        """æ ¼å¼åŒ–ç”Ÿæ´»å»ºè®®å“åº”"""
        # æ·»åŠ ç»“æ„åŒ–æ ‡é¢˜
        if 'ç”Ÿæ´»è°ƒç†' not in response and 'å»ºè®®' not in response:
            response = f"## ç”Ÿæ´»è°ƒç†å»ºè®®\n\n{response}"
        
        return response
    
    def _truncate_response_intelligently(self, response: str) -> str:
        """æ™ºèƒ½æˆªæ–­å“åº”"""
        # æŒ‰æ®µè½æˆªæ–­ï¼Œä¿æŒå®Œæ•´æ€§
        paragraphs = response.split('\n\n')
        
        truncated_paragraphs = []
        current_length = 0
        
        for paragraph in paragraphs:
            if current_length + len(paragraph) > 2800:  # ç•™ä¸€äº›ç©ºé—´ç»™ç»“å°¾
                break
            truncated_paragraphs.append(paragraph)
            current_length += len(paragraph)
        
        truncated_response = '\n\n'.join(truncated_paragraphs)
        
        # æ·»åŠ æˆªæ–­æç¤º
        if len(truncated_response) < len(response):
            truncated_response += '\n\n[å“åº”å†…å®¹è¾ƒé•¿ï¼Œå·²æ™ºèƒ½æˆªæ–­]'
        
        return truncated_response
    
    def _add_medical_disclaimer(self, response: str) -> str:
        """æ·»åŠ åŒ»ç–—å…è´£å£°æ˜"""
        disclaimer = "\n\n---\n**åŒ»ç–—å£°æ˜**: ä»¥ä¸Šå†…å®¹ä»…ä¾›å‚è€ƒå­¦ä¹ ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­ã€‚å¦‚æœ‰ç–¾ç—…ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚"
        
        return response + disclaimer
    
    def _evaluate_response_quality(self, response: str, query: str, 
                                  response_type: str) -> float:
        """è¯„ä¼°å“åº”è´¨é‡"""
        quality_score = 1.0
        
        # é•¿åº¦æ£€æŸ¥
        response_length = len(response)
        if response_length < self.quality_rules['min_length']:
            quality_score -= 0.3
        elif response_length > self.quality_rules['max_length']:
            quality_score -= 0.2
        
        # ç»“æ„å®Œæ•´æ€§æ£€æŸ¥
        required_sections = self.quality_rules.get('required_sections', [])
        for section in required_sections:
            if section not in response:
                quality_score -= 0.2
        
        # ç¦ç”¨è¯æ£€æŸ¥
        forbidden_words = self.quality_rules.get('forbidden_words', [])
        for word in forbidden_words:
            if word in response:
                quality_score -= 0.1
        
        # ä¸­åŒ»ä¸“ä¸šæ€§æ£€æŸ¥
        tcm_keywords = ['ä¸­åŒ»', 'è¯å‹', 'è¾¨è¯', 'è„‰è±¡', 'èˆŒè±¡', 'æ°”è¡€', 'é˜´é˜³', 'è„è…‘']
        tcm_keyword_count = sum(1 for kw in tcm_keywords if kw in response)
        if response_type == 'tcm_diagnosis' and tcm_keyword_count < 3:
            quality_score -= 0.2
        
        # æŸ¥è¯¢ç›¸å…³æ€§æ£€æŸ¥
        from ..utils.text_utils import extract_keywords_from_query
        query_keywords = extract_keywords_from_query(query)
        matched_keywords = sum(1 for kw in query_keywords if kw.lower() in response.lower())
        relevance_ratio = matched_keywords / len(query_keywords) if query_keywords else 1.0
        if relevance_ratio < 0.5:
            quality_score -= 0.3
        
        # ç¡®ä¿è´¨é‡åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        return max(0.0, min(quality_score, 1.0))
    
    def generate_follow_up_questions(self, conversation_context: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆåç»­é—®é¢˜å»ºè®®"""
        follow_up_questions = []
        
        try:
            # åŸºäºå¯¹è¯ç±»å‹ç”Ÿæˆä¸åŒçš„åç»­é—®é¢˜
            user_pattern = conversation_context.get('user_history', {}).get('user_pattern', {})
            most_common_type = user_pattern.get('most_common_type', '')
            
            question_templates = {
                'symptom_inquiry': [
                    "ç—‡çŠ¶å‡ºç°å¤šé•¿æ—¶é—´äº†ï¼Ÿ",
                    "æ˜¯å¦æœ‰å…¶ä»–ä¼´éšç—‡çŠ¶ï¼Ÿ",
                    "ä»€ä¹ˆæƒ…å†µä¸‹ç—‡çŠ¶ä¼šåŠ é‡ï¼Ÿ"
                ],
                'prescription_request': [
                    "æ‚¨æ˜¯å¦æœ‰è¯ç‰©è¿‡æ•å²ï¼Ÿ",
                    "ç›®å‰æ˜¯å¦åœ¨æœç”¨å…¶ä»–è¯ç‰©ï¼Ÿ",
                    "å¸Œæœ›äº†è§£ç”¨è¯æ³¨æ„äº‹é¡¹å—ï¼Ÿ"
                ],
                'lifestyle_advice': [
                    "æ‚¨çš„æ—¥å¸¸ä½œæ¯å¦‚ä½•ï¼Ÿ",
                    "é¥®é£Ÿä¹ æƒ¯æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
                    "å·¥ä½œå‹åŠ›å¤§å—ï¼Ÿ"
                ]
            }
            
            # æ ¹æ®å†å²æ¨¡å¼é€‰æ‹©é—®é¢˜
            if most_common_type in question_templates:
                follow_up_questions.extend(question_templates[most_common_type][:2])
            
            # åŸºäºå½“å‰ç—‡çŠ¶ç”Ÿæˆé’ˆå¯¹æ€§é—®é¢˜
            current_symptoms = conversation_context.get('symptoms', [])
            if 'å¤´ç—›' in current_symptoms:
                follow_up_questions.append("å¤´ç—›æ˜¯èƒ€ç—›è¿˜æ˜¯åˆºç—›ï¼Ÿ")
            elif 'å’³å—½' in current_symptoms:
                follow_up_questions.append("å’³å—½æ—¶æœ‰ç—°å—ï¼Ÿç—°çš„é¢œè‰²å¦‚ä½•ï¼Ÿ")
            
            # é€šç”¨åç»­é—®é¢˜
            if len(follow_up_questions) < 3:
                generic_questions = [
                    "è¿˜æœ‰ä»€ä¹ˆéœ€è¦äº†è§£çš„å—ï¼Ÿ",
                    "å¯¹å»ºè®®æœ‰ç–‘é—®å—ï¼Ÿ",
                    "éœ€è¦äº†è§£ç”¨è¯æ–¹æ³•å—ï¼Ÿ"
                ]
                follow_up_questions.extend(generic_questions[:3-len(follow_up_questions)])
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆåç»­é—®é¢˜å¤±è´¥: {e}")
            follow_up_questions = ["è¿˜æœ‰ä»€ä¹ˆéœ€è¦äº†è§£çš„å—ï¼Ÿ"]
        
        return follow_up_questions[:3]
    
    # æç¤ºè¯æ¨¡æ¿
    def _get_tcm_diagnosis_template(self) -> str:
        # è·å–å¤„æ–¹æ ¼å¼æç¤ºï¼ˆå¦‚æœéœ€è¦å¼€å¤„æ–¹çš„è¯ï¼‰
        try:
            prescription_enforcer = get_prescription_enforcer()
            format_prompt = prescription_enforcer.add_prescription_format_prompt()
        except:
            format_prompt = ""
            
        return f"""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸­åŒ»ä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„ç—‡çŠ¶æè¿°å’Œæä¾›çš„ä¿¡æ¯ï¼Œè¿›è¡Œä¸“ä¸šçš„ä¸­åŒ»è¯Šç–—åˆ†æã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š
1. ç—‡çŠ¶åˆ†æï¼šåˆ†æç”¨æˆ·æè¿°çš„ç—‡çŠ¶ç‰¹ç‚¹
2. è¯å‹åˆ¤æ–­ï¼šåŸºäºä¸­åŒ»ç†è®ºåˆ¤æ–­å¯èƒ½çš„è¯å‹
3. æ²»ç–—æ–¹æ¡ˆï¼šæä¾›ç›¸åº”çš„æ²»ç–—å»ºè®®
4. ç”Ÿæ´»è°ƒç†ï¼šç»™å‡ºæ—¥å¸¸è°ƒç†å»ºè®®

{format_prompt}

è¯·ç¡®ä¿å»ºè®®ä¸“ä¸šã€å®ç”¨ï¼Œç¬¦åˆä¸­åŒ»ç†è®ºã€‚"""
    
    def _get_prescription_template(self) -> str:
        # è·å–å¤„æ–¹æ ¼å¼æç¤º
        try:
            prescription_enforcer = get_prescription_enforcer()
            format_prompt = prescription_enforcer.add_prescription_format_prompt()
        except:
            format_prompt = ""
            
        return f"""ä½ æ˜¯ä¸€ä½ä¸­åŒ»å¤„æ–¹ä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„ç—‡çŠ¶å’Œè¯å‹ï¼Œå¼€å…·é€‚å½“çš„ä¸­è¯å¤„æ–¹ã€‚

{format_prompt}

è¯·åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. è¯å‹åˆ†æï¼šè¯´æ˜è¾¨è¯æ€è·¯
2. å¤„æ–¹ç»„æˆï¼šåˆ—å‡ºå…·ä½“è¯ç‰©å’Œç”¨é‡ï¼ˆå¿…é¡»åŒ…å«å…·ä½“å…‹æ•°ï¼‰
3. æ–¹è§£ï¼šè§£é‡Šå¤„æ–¹ç»„æˆåŸç†
4. æœç”¨æ–¹æ³•ï¼šè¯¦ç»†è¯´æ˜ç…ç…®å’Œæœç”¨æ–¹æ³•
5. æ³¨æ„äº‹é¡¹ï¼šç”¨è¯ç¦å¿Œå’Œæ³¨æ„äº‹é¡¹

è¯·ç¡®ä¿å¤„æ–¹å®‰å…¨ã€æœ‰æ•ˆï¼Œç¬¦åˆä¸­åŒ»å¤„æ–¹åŸåˆ™ã€‚"""
    
    def _get_symptom_analysis_template(self) -> str:
        return """ä½œä¸ºä¸­åŒ»ç—‡çŠ¶åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ç”¨æˆ·çš„ç—‡çŠ¶è¿›è¡Œè¯¦ç»†åˆ†æã€‚

åˆ†æè¦ç‚¹ï¼š
1. ç—‡çŠ¶ç‰¹å¾ï¼šæè¿°ç—‡çŠ¶çš„æ€§è´¨ã€ç¨‹åº¦ã€æ—¶é—´è§„å¾‹
2. ç—…å› åˆ†æï¼šä»ä¸­åŒ»è§’åº¦åˆ†æå¯èƒ½çš„ç—…å› 
3. è„è…‘å…³è”ï¼šåˆ†ææ¶‰åŠçš„è„è…‘ç³»ç»Ÿ
4. è¯å‹å€¾å‘ï¼šåˆæ­¥åˆ¤æ–­è¯å‹æ–¹å‘
5. è¿›ä¸€æ­¥æ£€æŸ¥å»ºè®®ï¼šå»ºè®®éœ€è¦å…³æ³¨çš„å…¶ä»–ç—‡çŠ¶

è¯·æä¾›ä¸“ä¸šã€å‡†ç¡®çš„ç—‡çŠ¶åˆ†æã€‚"""
    
    def _get_lifestyle_advice_template(self) -> str:
        return """ä½œä¸ºä¸­åŒ»å…»ç”Ÿä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·æƒ…å†µæä¾›å…¨é¢çš„ç”Ÿæ´»è°ƒç†å»ºè®®ã€‚

å»ºè®®åŒ…å«ï¼š
1. é¥®é£Ÿè°ƒç†ï¼šæ¨èé€‚å®œçš„é£Ÿç‰©å’Œé¥®é£Ÿä¹ æƒ¯
2. ä½œæ¯è°ƒæ•´ï¼šåˆç†çš„ä½œæ¯æ—¶é—´å®‰æ’
3. è¿åŠ¨å…»ç”Ÿï¼šé€‚å®œçš„è¿åŠ¨æ–¹å¼å’Œå¼ºåº¦
4. æƒ…å¿—è°ƒç†ï¼šæƒ…ç»ªè°ƒèŠ‚å’Œå¿ƒç†å¥åº·å»ºè®®
5. å­£èŠ‚å…»ç”Ÿï¼šç»“åˆå½“å‰å­£èŠ‚çš„ç‰¹æ®Šæ³¨æ„äº‹é¡¹

è¯·ç¡®ä¿å»ºè®®å®ç”¨ã€å¯æ“ä½œï¼Œç¬¦åˆä¸­åŒ»å…»ç”Ÿç†å¿µã€‚"""
    
    def _get_follow_up_template(self) -> str:
        return """åŸºäºä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·æä¾›åç»­å’¨è¯¢çš„å»ºè®®ã€‚

è¯·åŒ…å«ï¼š
1. æ²»ç–—æ•ˆæœè¯„ä¼°ï¼šè¯¢é—®æ²»ç–—æ•ˆæœå’Œå˜åŒ–
2. ç—‡çŠ¶è·Ÿè¸ªï¼šéœ€è¦æŒç»­å…³æ³¨çš„ç—‡çŠ¶
3. è°ƒç†å»ºè®®ï¼šåç»­è°ƒç†é‡ç‚¹
4. å¤è¯Šå»ºè®®ï¼šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥è¯Šç–—

è¯·æä¾›ä¸“ä¸šã€è´´å¿ƒçš„åç»­æŒ‡å¯¼ã€‚"""