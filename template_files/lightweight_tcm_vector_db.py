#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½»é‡çº§TCMå‘é‡æ•°æ®åº“åˆ›å»ºå™¨
ä¸ä¾èµ–ç½‘ç»œä¸‹è½½ï¼Œä½¿ç”¨TF-IDFå’Œä¸­æ–‡åˆ†è¯çš„æœ¬åœ°æ–¹æ¡ˆ
"""

import sys
import os
import json
import numpy as np
from typing import Dict, List, Any, Tuple
import logging
from datetime import datetime
import pickle
from collections import Counter, defaultdict
import math

# æ£€æŸ¥å¹¶å®‰è£…jieba
try:
    import jieba
    import jieba.analyse
except ImportError:
    os.system("pip install jieba")
    import jieba
    import jieba.analyse

# sklearnçš„TF-IDF
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    os.system("pip install scikit-learn")
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

sys.path.append('/opt/tcm-ai')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LightweightTCMVectorDatabase:
    """è½»é‡çº§TCMå‘é‡æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        print("ğŸ”§ åˆå§‹åŒ–è½»é‡çº§TCMå‘é‡æ•°æ®åº“...")
        
        # è®¾ç½®jiebaè‡ªå®šä¹‰è¯å…¸
        self._setup_custom_dict()
        
        # TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            tokenizer=self._jieba_tokenizer,
            lowercase=False,
            max_features=5000,  # æœ€å¤§ç‰¹å¾æ•°
            min_df=2,  # æœ€å°æ–‡æ¡£é¢‘ç‡
            max_df=0.8,  # æœ€å¤§æ–‡æ¡£é¢‘ç‡
            ngram_range=(1, 2)  # 1-2gram
        )
        
        # æ•°æ®å®¹å™¨
        self.prescriptions = []
        self.vectors = None
        self.metadata = {}
        
    def _setup_custom_dict(self):
        """è®¾ç½®è‡ªå®šä¹‰ä¸­åŒ»è¯å…¸"""
        # ä¸­åŒ»ä¸“ä¸šè¯æ±‡
        tcm_vocab = [
            # ç—…ç—‡
            'æ„Ÿå†’', 'å’³å—½', 'å‘çƒ­', 'å¤´ç—›', 'è…¹æ³»', 'ä¾¿ç§˜', 'å¤±çœ ', 'å’½ç‚',
            'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'æœˆç»å¤±è°ƒ', 'ç—›ç»', 'è‚¾ç‚', 'è‚ç¡¬åŒ–',
            # è¯å‹
            'é£å¯’æ„Ÿå†’', 'é£çƒ­æ„Ÿå†’', 'é˜´è™šç«æ—º', 'ç—°æ¹¿å’³å—½', 'è‚ºçƒ­å’³å—½',
            'è„¾è™šæ¹¿ç››', 'è‚éƒæ°”æ»', 'å¿ƒè¡€ä¸è¶³', 'è‚¾é˜³è™š', 'è‚¾é˜´è™š',
            # ç—‡çŠ¶
            'æ¶å¯’', 'å‘çƒ­', 'å’³ç—°', 'æ°”çŸ­', 'èƒ¸é—·', 'å¿ƒæ‚¸', 'ç›—æ±—',
            'å£å¹²', 'å’½ç—›', 'è…¹ç—›', 'è…°é…¸', 'å¤´æ™•', 'ä¹åŠ›',
            # æ²»æ³•
            'è¾›æ¸©è§£è¡¨', 'è¾›å‡‰è§£è¡¨', 'æ¸…çƒ­è§£æ¯’', 'åŒ–ç—°æ­¢å’³', 'æ¶¦è‚ºæ­¢å’³',
            'å¥è„¾ç›Šæ°”', 'ç–è‚ç†æ°”', 'æ´»è¡€åŒ–ç˜€', 'æ»‹é˜´æ¶¦ç‡¥', 'æ¸©é˜³åˆ©æ°´',
            # è¯ç‰©
            'ç”˜è‰', 'å½“å½’', 'ç™½èŠ', 'å·èŠ', 'èŒ¯è‹“', 'ç™½æœ¯', 'é™ˆçš®',
            'åŠå¤', 'é»„èŠ©', 'æŸ´èƒ¡', 'æ¡”æ¢—', 'æä»', 'éº»é»„', 'æ¡‚æ',
            # æ–¹å‰‚
            'é“¶ç¿˜æ•£', 'éº»æçŸ³ç”˜æ±¤', 'å°æŸ´èƒ¡æ±¤', 'å››å›å­æ±¤', 'å…­å‘³åœ°é»„ä¸¸'
        ]
        
        for word in tcm_vocab:
            jieba.add_word(word)
    
    def _jieba_tokenizer(self, text: str) -> List[str]:
        """ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯"""
        # åˆ†è¯
        words = jieba.lcut(text)
        
        # è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·å’Œå•å­—ç¬¦
        filtered_words = []
        for word in words:
            word = word.strip()
            if len(word) >= 2 and not self._is_punctuation(word):
                filtered_words.append(word)
        
        return filtered_words
    
    def _is_punctuation(self, word: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡ç‚¹ç¬¦å·"""
        punctuations = set('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€Â·ï½@#ï¿¥%â€¦â€¦&*+-={}[]|\\/')
        return all(c in punctuations or c.isspace() or c.isdigit() for c in word)
    
    def load_tcm_database(self, database_path: str) -> bool:
        """åŠ è½½TCMæ•°æ®åº“"""
        try:
            with open(database_path, 'r', encoding='utf-8') as f:
                self.tcm_data = json.load(f)
            
            print(f"âœ… æˆåŠŸåŠ è½½TCMæ•°æ®åº“")
            print(f"   - æ–‡æ¡£æ•°: {self.tcm_data['total_documents']}")
            print(f"   - å¤„æ–¹æ•°: {self.tcm_data['total_prescriptions']}")
            print(f"   - ç—…ç—‡æ•°: {len(self.tcm_data['diseases_index'])}")
            
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def prepare_prescription_texts(self) -> List[Dict[str, Any]]:
        """å‡†å¤‡å¤„æ–¹æ–‡æœ¬ç”¨äºå‘é‡åŒ–"""
        print("ğŸ“ å‡†å¤‡å¤„æ–¹æ–‡æœ¬æ•°æ®...")
        
        prescription_texts = []
        prescription_id = 0
        
        for doc in self.tcm_data['documents']:
            disease_name = doc['disease_name']
            
            for prescription in doc['prescriptions']:
                prescription_id += 1
                
                # æ„å»ºæœç´¢æ–‡æœ¬ - åŒ…å«å¤šä¸ªç»´åº¦çš„ä¿¡æ¯
                text_parts = []
                
                # 1. ç—…ç—‡åç§°
                text_parts.append(disease_name)
                
                # 2. è¯å‹
                if prescription['syndrome']:
                    text_parts.append(prescription['syndrome'])
                
                # 3. æ²»æ³•
                if prescription['treatment_method']:
                    text_parts.append(prescription['treatment_method'])
                
                # 4. æ–¹å‰‚åç§°
                text_parts.append(prescription['formula_name'])
                
                # 5. è¯ç‰©åç§°
                herb_names = [herb['name'] for herb in prescription['herbs']]
                text_parts.extend(herb_names)
                
                # 6. ç—‡çŠ¶å…³é”®è¯ï¼ˆä»åŸæ–‡ä¸­æå–ï¼‰
                symptoms = self._extract_symptom_keywords(prescription['source_text'])
                text_parts.extend(symptoms)
                
                # åˆå¹¶æˆæœç´¢æ–‡æœ¬
                search_text = ' '.join(text_parts)
                
                # åˆ›å»ºæ˜¾ç¤ºæ–‡æœ¬ï¼ˆæ›´æ˜“è¯»ï¼‰
                display_parts = []
                display_parts.append(f"ç—…ç—‡ï¼š{disease_name}")
                
                if prescription['syndrome']:
                    display_parts.append(f"è¯å‹ï¼š{prescription['syndrome']}")
                
                if prescription['treatment_method']:
                    display_parts.append(f"æ²»æ³•ï¼š{prescription['treatment_method']}")
                
                display_parts.append(f"æ–¹å‰‚ï¼š{prescription['formula_name']}")
                
                herbs_text = "è¯ç‰©ï¼š"
                herb_list = []
                for herb in prescription['herbs'][:10]:  # æ˜¾ç¤ºå‰10å‘³è¯
                    herb_list.append(f"{herb['name']}{herb['dose']}{herb['unit']}")
                herbs_text += "ã€".join(herb_list)
                if len(prescription['herbs']) > 10:
                    herbs_text += f"ç­‰{len(prescription['herbs'])}å‘³è¯"
                display_parts.append(herbs_text)
                
                if prescription['usage']:
                    display_parts.append(f"ç”¨æ³•ï¼š{prescription['usage']}")
                
                display_text = "ï¼›".join(display_parts)
                
                # åˆ›å»ºå¤„æ–¹è®°å½•
                prescription_record = {
                    'id': prescription_id,
                    'disease_name': disease_name,
                    'syndrome': prescription['syndrome'],
                    'treatment_method': prescription['treatment_method'],
                    'formula_name': prescription['formula_name'],
                    'herbs': prescription['herbs'],
                    'herb_count': prescription['herb_count'],
                    'usage': prescription['usage'],
                    'source_document': doc['filename'],
                    'source_text': prescription['source_text'],
                    'search_text': search_text,
                    'display_text': display_text,
                    'search_keywords': self._generate_search_keywords(prescription, disease_name)
                }
                
                prescription_texts.append(prescription_record)
        
        print(f"âœ… å‡†å¤‡äº† {len(prescription_texts)} æ¡å¤„æ–¹æ–‡æœ¬")
        return prescription_texts
    
    def _extract_symptom_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–ç—‡çŠ¶å…³é”®è¯"""
        symptom_keywords = [
            'å‘çƒ­', 'æ¶å¯’', 'å’³å—½', 'ç—°å¤š', 'ç—°å°‘', 'ç—°é»„', 'ç—°ç™½', 'ç—°ç²˜',
            'æ°”çŸ­', 'èƒ¸é—·', 'èƒ¸ç—›', 'å¿ƒæ‚¸', 'å¿ƒçƒ¦', 'å¤±çœ ', 'å¤šæ¢¦',
            'å¤´ç—›', 'å¤´æ™•', 'ç›®çœ©', 'è€³é¸£', 'å’½ç—›', 'å’½å¹²', 'å£å¹²',
            'å£è‹¦', 'å£æ·¡', 'æ¶å¿ƒ', 'å‘•å', 'è…¹ç—›', 'è…¹èƒ€', 'è…¹æ³»',
            'ä¾¿ç§˜', 'ä¾¿è¡€', 'å°¿é¢‘', 'å°¿æ€¥', 'å°¿ç—›', 'æ°´è‚¿', 'ä¹åŠ›',
            'ç–²å€¦', 'è‡ªæ±—', 'ç›—æ±—', 'æ½®çƒ­', 'æ€•å†·', 'è…°é…¸', 'è…°ç—›',
            'è†è½¯', 'æœˆç»ä¸è°ƒ', 'ç—›ç»', 'ç™½å¸¦', 'é˜³ç—¿', 'æ—©æ³„',
            'èˆŒçº¢', 'èˆŒæ·¡', 'èˆŒèƒ–', 'è‹”è–„', 'è‹”åš', 'è‹”é»„', 'è‹”ç™½',
            'è‹”è…»', 'è„‰æ•°', 'è„‰ç¼“', 'è„‰ç»†', 'è„‰æ»‘', 'è„‰å¼¦', 'è„‰æ²‰'
        ]
        
        found_symptoms = []
        for symptom in symptom_keywords:
            if symptom in text:
                found_symptoms.append(symptom)
        
        return found_symptoms[:15]  # æœ€å¤šè¿”å›15ä¸ªç—‡çŠ¶
    
    def _generate_search_keywords(self, prescription: Dict, disease_name: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        keywords = []
        
        # ç—…ç—‡åç§°åŠå…¶åˆ†è¯
        keywords.append(disease_name)
        disease_words = jieba.lcut(disease_name)
        keywords.extend([w for w in disease_words if len(w) >= 2])
        
        # è¯å‹
        if prescription['syndrome']:
            keywords.append(prescription['syndrome'])
            syndrome_words = jieba.lcut(prescription['syndrome'])
            keywords.extend([w for w in syndrome_words if len(w) >= 2])
        
        # æ²»æ³•
        if prescription['treatment_method']:
            treatment_words = jieba.lcut(prescription['treatment_method'])
            keywords.extend([w for w in treatment_words if len(w) >= 2])
        
        # æ–¹å‰‚åç§°
        keywords.append(prescription['formula_name'])
        
        # ä¸»è¦è¯ç‰©
        for herb in prescription['herbs'][:8]:  # å‰8å‘³è¯
            keywords.append(herb['name'])
        
        # å»é‡å¹¶è¿‡æ»¤
        keywords = list(set([kw for kw in keywords if len(kw) >= 2]))
        
        return keywords
    
    def create_vectors(self, prescription_texts: List[Dict[str, Any]]) -> np.ndarray:
        """åˆ›å»ºTF-IDFå‘é‡"""
        print("ğŸ§® åˆ›å»ºTF-IDFå‘é‡...")
        
        texts = [item['search_text'] for item in prescription_texts]
        
        # è®­ç»ƒTF-IDFæ¨¡å‹å¹¶è½¬æ¢æ–‡æœ¬
        vectors = self.vectorizer.fit_transform(texts)
        
        print(f"âœ… åˆ›å»ºäº† {vectors.shape[0]} ä¸ªå‘é‡ï¼Œç»´åº¦: {vectors.shape[1]}")
        print(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {len(self.vectorizer.vocabulary_)}")
        
        return vectors
    
    def save_vector_database(self, output_dir: str):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        print(f"ğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“åˆ°: {output_dir}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜TF-IDFå‘é‡ï¼ˆç¨€ç–çŸ©é˜µï¼‰
        with open(os.path.join(output_dir, 'tfidf_vectors.pkl'), 'wb') as f:
            pickle.dump(self.vectors, f)
        
        # ä¿å­˜TF-IDFå‘é‡åŒ–å™¨
        with open(os.path.join(output_dir, 'tfidf_vectorizer.pkl'), 'wb') as f:
            pickle.dump(self.vectorizer, f)
        
        # ä¿å­˜å¤„æ–¹æ•°æ®
        with open(os.path.join(output_dir, 'prescriptions.json'), 'w', encoding='utf-8') as f:
            json.dump(self.prescriptions, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'creation_time': datetime.now().isoformat(),
            'model_type': 'TF-IDF',
            'total_prescriptions': len(self.prescriptions),
            'vector_dimension': self.vectors.shape[1] if self.vectors is not None else 0,
            'vocabulary_size': len(self.vectorizer.vocabulary_),
            'diseases_count': len(set([p['disease_name'] for p in self.prescriptions])),
            'herbs_count': len(set([herb['name'] for p in self.prescriptions for herb in p['herbs']])),
            'similarity_metric': 'cosine'
        }
        
        with open(os.path.join(output_dir, 'metadata.json'), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print("âœ… å‘é‡æ•°æ®åº“ä¿å­˜å®Œæˆ")
        
        # ç”Ÿæˆç®€å•çš„æµ‹è¯•è„šæœ¬
        test_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TCMå‘é‡æ•°æ®åº“æµ‹è¯•è„šæœ¬
"""
import pickle
import json
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import jieba

def load_database():
    """åŠ è½½æ•°æ®åº“"""
    # åŠ è½½å‘é‡
    with open('tfidf_vectors.pkl', 'rb') as f:
        vectors = pickle.load(f)
    
    # åŠ è½½å‘é‡åŒ–å™¨
    with open('tfidf_vectorizer.pkl', 'rb') as f:
        vectorizer = pickle.load(f)
    
    # åŠ è½½å¤„æ–¹æ•°æ®
    with open('prescriptions.json', 'r', encoding='utf-8') as f:
        prescriptions = json.load(f)
    
    return vectors, vectorizer, prescriptions

def search_prescriptions(query, k=5):
    """æœç´¢ç›¸ä¼¼å¤„æ–¹"""
    vectors, vectorizer, prescriptions = load_database()
    
    # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
    query_vector = vectorizer.transform([query])
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = cosine_similarity(query_vector, vectors)[0]
    
    # è·å–top-kç»“æœ
    top_indices = np.argsort(similarities)[::-1][:k]
    
    results = []
    for i, idx in enumerate(top_indices):
        result = {
            'rank': i + 1,
            'similarity': similarities[idx],
            'prescription': prescriptions[idx]
        }
        results.append(result)
    
    return results

if __name__ == "__main__":
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "å’³å—½ç—°å¤šå‘çƒ­",
        "é£å¯’æ„Ÿå†’æ¶å¯’",
        "å¤´ç—›å¤±çœ å¿ƒçƒ¦",
        "è…¹æ³»è…¹ç—›"
    ]
    
    print("ğŸ” TCMå‘é‡æ•°æ®åº“æµ‹è¯•")
    print("=" * 60)
    
    for test_query in test_queries:
        print(f"\\næŸ¥è¯¢: {test_query}")
        results = search_prescriptions(test_query, k=3)
        
        for result in results:
            print(f"  [{result['rank']}] ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
            print(f"      {result['prescription']['display_text'][:100]}...")
'''
        
        with open(os.path.join(output_dir, 'test_search.py'), 'w', encoding='utf-8') as f:
            f.write(test_script)
        
        # ç”Ÿæˆä½¿ç”¨è¯´æ˜
        readme_content = f"""# TCMè½»é‡çº§å‘é‡æ•°æ®åº“

## æ•°æ®åº“ä¿¡æ¯
- åˆ›å»ºæ—¶é—´: {metadata['creation_time']}
- æ¨¡å‹ç±»å‹: {metadata['model_type']}
- å¤„æ–¹æ€»æ•°: {metadata['total_prescriptions']}
- å‘é‡ç»´åº¦: {metadata['vector_dimension']}
- è¯æ±‡è¡¨å¤§å°: {metadata['vocabulary_size']}
- ç—…ç—‡æ•°é‡: {metadata['diseases_count']}
- è¯ç‰©æ•°é‡: {metadata['herbs_count']}

## æ–‡ä»¶è¯´æ˜
- `tfidf_vectors.pkl`: TF-IDFå‘é‡æ•°æ®ï¼ˆscipyç¨€ç–çŸ©é˜µï¼‰
- `tfidf_vectorizer.pkl`: TF-IDFå‘é‡åŒ–å™¨
- `prescriptions.json`: å¤„æ–¹è¯¦ç»†æ•°æ®
- `metadata.json`: å…ƒæ•°æ®ä¿¡æ¯
- `test_search.py`: æµ‹è¯•æœç´¢è„šæœ¬

## å¿«é€Ÿæµ‹è¯•
```bash
python test_search.py
```

## ä½¿ç”¨ç¤ºä¾‹
```python
import pickle
import json
from sklearn.metrics.pairwise import cosine_similarity

# åŠ è½½æ•°æ®åº“
with open('tfidf_vectors.pkl', 'rb') as f:
    vectors = pickle.load(f)

with open('tfidf_vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

with open('prescriptions.json', 'r', encoding='utf-8') as f:
    prescriptions = json.load(f)

# æœç´¢
query = "å’³å—½ç—°å¤šå‘çƒ­"
query_vector = vectorizer.transform([query])
similarities = cosine_similarity(query_vector, vectors)[0]
top_indices = similarities.argsort()[::-1][:5]

# æ˜¾ç¤ºç»“æœ
for i, idx in enumerate(top_indices):
    print(f"{{i+1}}. ç›¸ä¼¼åº¦: {{similarities[idx]:.3f}}")
    print(f"   {{prescriptions[idx]['display_text']}}")
```

## ä¼˜åŠ¿
1. å®Œå…¨æœ¬åœ°åŒ–ï¼Œæ— éœ€ç½‘ç»œè¿æ¥
2. è½»é‡çº§ï¼Œæ–‡ä»¶å¤§å°é€‚ä¸­
3. å¿«é€Ÿæœç´¢ï¼Œæ¯«ç§’çº§å“åº”
4. æ”¯æŒä¸­æ–‡åˆ†è¯å’Œè¯­ä¹‰åŒ¹é…
5. æ˜“äºé›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ
"""
        
        with open(os.path.join(output_dir, 'README.md'), 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    def create_complete_database(self, database_path: str, output_dir: str):
        """åˆ›å»ºå®Œæ•´çš„å‘é‡æ•°æ®åº“"""
        print("ğŸš€ å¼€å§‹åˆ›å»ºè½»é‡çº§TCMå‘é‡æ•°æ®åº“...")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_tcm_database(database_path):
            return False
        
        # 2. å‡†å¤‡æ–‡æœ¬
        self.prescriptions = self.prepare_prescription_texts()
        
        # 3. åˆ›å»ºå‘é‡
        self.vectors = self.create_vectors(self.prescriptions)
        
        # 4. ä¿å­˜æ•°æ®åº“
        self.save_vector_database(output_dir)
        
        print("ğŸ‰ è½»é‡çº§TCMå‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("è½»é‡çº§TCMç—…ç—‡-å¤„æ–¹å‘é‡æ•°æ®åº“åˆ›å»ºå™¨")
    print("=" * 80)
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    db_creator = LightweightTCMVectorDatabase()
    
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    database_path = '/opt/tcm-ai/template_files/complete_tcm_database.json'
    output_dir = '/opt/tcm-ai/template_files/lightweight_vector_db'
    
    # åˆ›å»ºæ•°æ®åº“
    success = db_creator.create_complete_database(database_path, output_dir)
    
    if success:
        print(f"\nâœ… è½»é‡çº§å‘é‡æ•°æ®åº“å·²æˆåŠŸåˆ›å»ºåœ¨: {output_dir}")
        print("ğŸ” è¿è¡Œæµ‹è¯•: python test_search.py")
        print("ğŸ“– æŸ¥çœ‹è¯´æ˜: cat README.md")
    else:
        print("\nâŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥")
    
    return success

if __name__ == "__main__":
    main()