#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TCMå‘é‡æ•°æ®åº“åˆ›å»ºå™¨
åŸºäºæå–çš„ç—…ç—‡-å¤„æ–¹æ•°æ®åˆ›å»ºä¼˜åŒ–çš„å‘é‡æ•°æ®åº“ï¼Œç”¨äºé—®è¯Šåº”ç”¨
"""

import sys
import os
import json
import numpy as np
from typing import Dict, List, Any, Tuple
import logging
from datetime import datetime
import pickle

# æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„åŒ…
try:
    import jieba
    import jieba.analyse
except ImportError:
    os.system("pip install jieba")
    import jieba
    import jieba.analyse

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    os.system("pip install sentence-transformers")
    from sentence_transformers import SentenceTransformer

try:
    import faiss
except ImportError:
    os.system("pip install faiss-cpu")
    import faiss

sys.path.append('/opt/tcm-ai')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TCMVectorDatabase:
    """TCMå‘é‡æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        print("ğŸ”§ åˆå§‹åŒ–TCMå‘é‡æ•°æ®åº“...")
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        try:
            self.model = SentenceTransformer(model_name)
            print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
            print("âœ… ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: all-MiniLM-L6-v2")
        
        # è®¾ç½®jieba
        jieba.set_dictionary('dict.txt') if os.path.exists('dict.txt') else None
        
        # æ•°æ®å®¹å™¨
        self.prescriptions = []
        self.vectors = None
        self.index = None
        self.metadata = {}
        
        # å‘é‡ç»´åº¦
        self.vector_dim = 384  # MiniLMæ¨¡å‹çš„å‘é‡ç»´åº¦
        
    def load_tcm_database(self, database_path: str) -> bool:
        """åŠ è½½TCMæ•°æ®åº“"""
        try:
            with open(database_path, 'r', encoding='utf-8') as f:
                self.tcm_data = json.load(f)
            
            print(f"âœ… æˆåŠŸåŠ è½½TCMæ•°æ®åº“")
            print(f"   - æ–‡æ¡£æ•°: {self.tcm_data['total_documents']}")
            print(f"   - å¤„æ–¹æ•°: {self.tcm_data['total_prescriptions']}")
            print(f"   - ç—…ç—‡æ•°: {len(self.tcm_data['diseases_index'])}")
            
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def prepare_prescription_texts(self) -> List[Dict[str, Any]]:
        """å‡†å¤‡å¤„æ–¹æ–‡æœ¬ç”¨äºå‘é‡åŒ–"""
        print("ğŸ“ å‡†å¤‡å¤„æ–¹æ–‡æœ¬æ•°æ®...")
        
        prescription_texts = []
        prescription_id = 0
        
        for doc in self.tcm_data['documents']:
            disease_name = doc['disease_name']
            
            for prescription in doc['prescriptions']:
                prescription_id += 1
                
                # æ„å»ºå®Œæ•´çš„å¤„æ–¹æè¿°æ–‡æœ¬
                text_parts = []
                
                # 1. ç—…ç—‡åç§°
                text_parts.append(f"ç—…ç—‡ï¼š{disease_name}")
                
                # 2. è¯å‹
                if prescription['syndrome']:
                    text_parts.append(f"è¯å‹ï¼š{prescription['syndrome']}")
                
                # 3. æ²»æ³•
                if prescription['treatment_method']:
                    text_parts.append(f"æ²»æ³•ï¼š{prescription['treatment_method']}")
                
                # 4. æ–¹å‰‚åç§°
                text_parts.append(f"æ–¹å‰‚ï¼š{prescription['formula_name']}")
                
                # 5. è¯ç‰©ç»„æˆ
                herbs_text = "è¯ç‰©ç»„æˆï¼š"
                herb_names = []
                for herb in prescription['herbs']:
                    herb_names.append(f"{herb['name']}{herb['dose']}{herb['unit']}")
                herbs_text += "ã€".join(herb_names)
                text_parts.append(herbs_text)
                
                # 6. ç”¨æ³•
                if prescription['usage']:
                    text_parts.append(f"ç”¨æ³•ï¼š{prescription['usage']}")
                
                # 7. ç—‡çŠ¶æè¿°ï¼ˆä»åŸæ–‡ä¸­æå–å…³é”®è¯ï¼‰
                symptoms = self._extract_symptoms_from_text(prescription['source_text'])
                if symptoms:
                    text_parts.append(f"ä¸»è¦ç—‡çŠ¶ï¼š{symptoms}")
                
                # åˆå¹¶æˆå®Œæ•´æ–‡æœ¬
                full_text = "ï¼›".join(text_parts)
                
                # åˆ›å»ºå¤„æ–¹è®°å½•
                prescription_record = {
                    'id': prescription_id,
                    'disease_name': disease_name,
                    'syndrome': prescription['syndrome'],
                    'treatment_method': prescription['treatment_method'],
                    'formula_name': prescription['formula_name'],
                    'herbs': prescription['herbs'],
                    'herb_count': prescription['herb_count'],
                    'usage': prescription['usage'],
                    'source_document': doc['filename'],
                    'source_text': prescription['source_text'],
                    'full_text': full_text,
                    'search_keywords': self._generate_search_keywords(prescription, disease_name)
                }
                
                prescription_texts.append(prescription_record)
        
        print(f"âœ… å‡†å¤‡äº† {len(prescription_texts)} æ¡å¤„æ–¹æ–‡æœ¬")
        return prescription_texts
    
    def _extract_symptoms_from_text(self, text: str) -> str:
        """ä»åŸæ–‡ä¸­æå–ç—‡çŠ¶å…³é”®è¯"""
        # å¸¸è§ç—‡çŠ¶å…³é”®è¯
        symptom_keywords = [
            'å‘çƒ­', 'æ¶å¯’', 'å’³å—½', 'ç—°å¤š', 'æ°”çŸ­', 'èƒ¸é—·', 'å¤´ç—›', 'å¤´æ™•',
            'æ¶å¿ƒ', 'å‘•å', 'è…¹ç—›', 'è…¹æ³»', 'ä¾¿ç§˜', 'å°¿é¢‘', 'å°¿æ€¥', 'å°¿ç—›',
            'å¿ƒæ‚¸', 'å¤±çœ ', 'å¤šæ¢¦', 'ç›—æ±—', 'è‡ªæ±—', 'ä¹åŠ›', 'ç–²å€¦',
            'é£Ÿæ¬²ä¸æŒ¯', 'å£å¹²', 'å£è‹¦', 'å’½ç—›', 'é¼»å¡', 'æµæ¶•',
            'æœˆç»ä¸è°ƒ', 'ç—›ç»', 'é—­ç»', 'ç™½å¸¦', 'é˜³ç—¿', 'æ—©æ³„',
            'è…°é…¸', 'è…°ç—›', 'è†è½¯', 'æ‰‹è¶³å†°å†·', 'æ½®çƒ­', 'çƒ¦èº',
            'èˆŒçº¢', 'èˆŒæ·¡', 'è‹”è–„', 'è‹”åš', 'è‹”é»„', 'è‹”ç™½', 'è„‰æ•°', 'è„‰ç¼“'
        ]
        
        found_symptoms = []
        for symptom in symptom_keywords:
            if symptom in text:
                found_symptoms.append(symptom)
        
        return "ã€".join(found_symptoms[:10])  # æœ€å¤šè¿”å›10ä¸ªç—‡çŠ¶
    
    def _generate_search_keywords(self, prescription: Dict, disease_name: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        keywords = []
        
        # ç—…ç—‡åç§°
        keywords.append(disease_name)
        
        # è¯å‹
        if prescription['syndrome']:
            keywords.append(prescription['syndrome'])
            # æ‹†åˆ†è¯å‹çš„ç»„æˆéƒ¨åˆ†
            syndrome_parts = jieba.lcut(prescription['syndrome'])
            keywords.extend([part for part in syndrome_parts if len(part) >= 2])
        
        # æ²»æ³•
        if prescription['treatment_method']:
            treatment_parts = jieba.lcut(prescription['treatment_method'])
            keywords.extend([part for part in treatment_parts if len(part) >= 2])
        
        # æ–¹å‰‚åç§°
        keywords.append(prescription['formula_name'])
        
        # ä¸»è¦è¯ç‰©
        for herb in prescription['herbs'][:10]:  # å‰10å‘³è¯
            keywords.append(herb['name'])
        
        # å»é‡å¹¶è¿‡æ»¤çŸ­è¯
        keywords = list(set([kw for kw in keywords if len(kw) >= 2]))
        
        return keywords
    
    def create_vectors(self, prescription_texts: List[Dict[str, Any]]) -> np.ndarray:
        """åˆ›å»ºå‘é‡"""
        print("ğŸ§® åˆ›å»ºå‘é‡åµŒå…¥...")
        
        texts = [item['full_text'] for item in prescription_texts]
        
        # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜ä¸è¶³
        batch_size = 32
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)
            all_embeddings.append(batch_embeddings)
            
            if (i // batch_size + 1) % 10 == 0:
                print(f"   å·²å¤„ç†: {i + len(batch_texts)}/{len(texts)}")
        
        vectors = np.vstack(all_embeddings)
        print(f"âœ… åˆ›å»ºäº† {vectors.shape[0]} ä¸ªå‘é‡ï¼Œç»´åº¦: {vectors.shape[1]}")
        
        return vectors
    
    def build_faiss_index(self, vectors: np.ndarray) -> faiss.Index:
        """æ„å»ºFAISSç´¢å¼•"""
        print("ğŸ—ï¸ æ„å»ºFAISSç´¢å¼•...")
        
        # ä½¿ç”¨IVFç´¢å¼•æé«˜æœç´¢æ•ˆç‡
        nlist = min(100, vectors.shape[0] // 10)  # èšç±»æ•°é‡
        quantizer = faiss.IndexFlatIP(vectors.shape[1])  # å†…ç§¯ç›¸ä¼¼åº¦
        index = faiss.IndexIVFFlat(quantizer, vectors.shape[1], nlist)
        
        # è®­ç»ƒç´¢å¼•
        index.train(vectors.astype('float32'))
        
        # æ·»åŠ å‘é‡
        index.add(vectors.astype('float32'))
        
        # è®¾ç½®æœç´¢å‚æ•°
        index.nprobe = min(10, nlist)
        
        print(f"âœ… FAISSç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {index.ntotal} ä¸ªå‘é‡")
        return index
    
    def save_vector_database(self, output_dir: str):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        print(f"ğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“åˆ°: {output_dir}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, os.path.join(output_dir, 'tcm_prescriptions.index'))
        
        # ä¿å­˜å¤„æ–¹æ•°æ®
        with open(os.path.join(output_dir, 'prescriptions.json'), 'w', encoding='utf-8') as f:
            json.dump(self.prescriptions, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'creation_time': datetime.now().isoformat(),
            'model_name': self.model.get_sentence_embedding_dimension(),
            'total_prescriptions': len(self.prescriptions),
            'vector_dimension': self.vector_dim,
            'diseases_count': len(set([p['disease_name'] for p in self.prescriptions])),
            'herbs_count': len(set([herb['name'] for p in self.prescriptions for herb in p['herbs']])),
            'index_type': 'IVFFlat',
            'similarity_metric': 'inner_product'
        }
        
        with open(os.path.join(output_dir, 'metadata.json'), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å‘é‡ï¼ˆç”¨äºå¤‡ä»½ï¼‰
        np.save(os.path.join(output_dir, 'vectors.npy'), self.vectors)
        
        print("âœ… å‘é‡æ•°æ®åº“ä¿å­˜å®Œæˆ")
        
        # ç”Ÿæˆä½¿ç”¨è¯´æ˜
        readme_content = f"""# TCMå‘é‡æ•°æ®åº“ä½¿ç”¨è¯´æ˜

## æ•°æ®åº“ä¿¡æ¯
- åˆ›å»ºæ—¶é—´: {metadata['creation_time']}
- å¤„æ–¹æ€»æ•°: {metadata['total_prescriptions']}
- ç—…ç—‡æ•°é‡: {metadata['diseases_count']}
- è¯ç‰©æ•°é‡: {metadata['herbs_count']}
- å‘é‡ç»´åº¦: {metadata['vector_dimension']}

## æ–‡ä»¶è¯´æ˜
- `tcm_prescriptions.index`: FAISSç´¢å¼•æ–‡ä»¶
- `prescriptions.json`: å¤„æ–¹è¯¦ç»†æ•°æ®
- `vectors.npy`: å‘é‡æ•°ç»„
- `metadata.json`: å…ƒæ•°æ®ä¿¡æ¯

## ä½¿ç”¨ç¤ºä¾‹
```python
import faiss
import json

# åŠ è½½ç´¢å¼•
index = faiss.read_index('tcm_prescriptions.index')

# åŠ è½½å¤„æ–¹æ•°æ®
with open('prescriptions.json', 'r', encoding='utf-8') as f:
    prescriptions = json.load(f)

# æœç´¢ç›¸ä¼¼å¤„æ–¹
query_vector = model.encode(["æ‚£è€…å’³å—½ç—°å¤šï¼ŒèˆŒè‹”é»„è…»"])
D, I = index.search(query_vector, k=5)

# è·å–ç»“æœ
results = [prescriptions[i] for i in I[0]]
```

## ä¼˜åŒ–ç‰¹ç‚¹
1. åŸºäºå®é™…TCMæ–‡æ¡£æå–çš„{metadata['total_prescriptions']}ä¸ªå¤„æ–¹
2. åŒ…å«å®Œæ•´çš„ç—…ç—‡-è¯å‹-æ²»æ³•-å¤„æ–¹æ˜ å°„
3. æ”¯æŒç—‡çŠ¶ã€è¯ç‰©ã€æ–¹å‰‚åç§°ç­‰å¤šç»´åº¦æœç´¢
4. ä½¿ç”¨é«˜æ•ˆçš„FAISSç´¢å¼•ï¼Œæ”¯æŒå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢
5. ä¸­æ–‡ä¼˜åŒ–çš„å‘é‡åŒ–å¤„ç†
"""
        
        with open(os.path.join(output_dir, 'README.md'), 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    def create_complete_database(self, database_path: str, output_dir: str):
        """åˆ›å»ºå®Œæ•´çš„å‘é‡æ•°æ®åº“"""
        print("ğŸš€ å¼€å§‹åˆ›å»ºTCMå‘é‡æ•°æ®åº“...")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_tcm_database(database_path):
            return False
        
        # 2. å‡†å¤‡æ–‡æœ¬
        self.prescriptions = self.prepare_prescription_texts()
        
        # 3. åˆ›å»ºå‘é‡
        self.vectors = self.create_vectors(self.prescriptions)
        
        # 4. æ„å»ºç´¢å¼•
        self.index = self.build_faiss_index(self.vectors)
        
        # 5. ä¿å­˜æ•°æ®åº“
        self.save_vector_database(output_dir)
        
        print("ğŸ‰ TCMå‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("TCMç—…ç—‡-å¤„æ–¹å‘é‡æ•°æ®åº“åˆ›å»ºå™¨")
    print("=" * 80)
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    db_creator = TCMVectorDatabase()
    
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    database_path = '/opt/tcm-ai/template_files/complete_tcm_database.json'
    output_dir = '/opt/tcm-ai/template_files/tcm_vector_db'
    
    # åˆ›å»ºæ•°æ®åº“
    success = db_creator.create_complete_database(database_path, output_dir)
    
    if success:
        print(f"\nâœ… å‘é‡æ•°æ®åº“å·²æˆåŠŸåˆ›å»ºåœ¨: {output_dir}")
        print("ğŸ” å¯ä»¥å¼€å§‹ç”¨äºé—®è¯Šåº”ç”¨äº†ï¼")
    else:
        print("\nâŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥")
    
    return success

if __name__ == "__main__":
    main()